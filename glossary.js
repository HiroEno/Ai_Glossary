// glossaryファイル Ai
`use strict`
const glossary = [
{target:`リスト`, content:`要素を順序づけて格納するデータ [1,2,3]。`},
{target:`辞書`, content:`キーと値のペアを保持するデータ {“名前”:”太郎”}。`},
{target:`タプル`, content:`不変な要素の順序付きデータ （1920,1080）。`},
{target:`セット`, content:`値の重複を認めないデータ {“A君”, “B君”,”Cさん”}。`},
{target:`int`, content:`整数型のデータ 1, -4, 100。`},
{target:`float`, content:`小数型のデータ 0.5, 3.14, 3.0。`},
{target:`str`, content:`文字列型のデータ “Hello”, ”30”。`},
{target:`関数`, content:`一連の処理をまとめたもの。defブロックで定義。`},
{target:`クラス`, content:`データやメソッドをまとめた設計図。オブジェクト指向プログラミングで使用。`},
{target:`インスタンス`, content:`クラスから生成された具体的なオブジェクト。`},
{target:`__init__`, content:`クラスのインスタンスが生成される際呼び出される特殊メソッド。初期化処理を行う。`},
{target:`メソッド`, content:`クラスやオブジェクトに属する関数。クラス内で定義された関数のこと。`},
{target:`ライブラリ`, content:`プログラミングする際に必要な機能をまとめたもの。`},
{target:`Numpy`, content:`数値計算を効率的に行うためのPythonのライブラリ。`},
{target:`Pandas`, content:`データ解析を支援するPythonのライブラリ。`},
{target:`Matplotlib`, content:`グラフ描画ライブラリ。`},
{target:`Seaborn`, content:`高度なグラフ描画ライブラリ。`},
{target:`ハッシュ値`, content:`元データから取り出した固定長の値。`},
{target:`真陽性 TP`, content:`ポジティブで、モデルが正しくポジティブと予測した数。実際◯、予測◯。`},
{target:`偽陽性 FP`, content:`ネガティブで、モデルが誤ってポジティブと予測した数。実際×、予測◯。`},
{target:`偽陰性 FN`, content:`ポジティブで、モデルが誤ってネガティブと予測した数。実際◯、予測×。`},
{target:`真陰性 TN`, content:`ネガティブで、モデルが正しくネガティブと予測した数。実際×、予測×。`},
{target:`正解率 accuracy`, content:`全データに対する正確な予測の割合。<br>（TP + TN） / 全データ。`},
{target:`適合率 precision`, content:`モデルが正と予測した中で、実際に正であったものの割合。<br>TP /（TP + FP）。precision_score（）関数。`},
{target:`再現率 recall`, content:`実際に正であるものの中で、正だと予測できた割合。<br>TP /（TP + FN）。recall_score（）関数。`},
{target:`真陽性率 TPR`, content:`再現率と同じく、実際に正であるものの中で、正だと予測できた割合。<br>TP /（TP + FN）。`},
{target:`偽陽性率 FPR`, content:`モデルが誤ってポジティブと予測した中で、実際にはネガティブであった割合。<br>FP /（FP + TN）。`},
{target:`F値`, content:`適合率と再現率の調和平均。<br>2 × precision × recall / precision + recall。f1_score（）関数。`},
{target:`ROC曲線`, content:`二値分類タスク（問題）に対する評価指標。<br>縦軸に真陽性率（TPR、True Positive Rate）、横軸に偽陽性率（FPR、False Positive Rate）をプロット。roc_curve（）関数。`},
{target:`AUC`, content:`ROC曲線の下の面積。<br>1.0に近づくほど性能が良い。roc_auc_score（）関数。`},
{target:`AIC`, content:`赤池情報量規準。<br>統計的モデルの予測性の良さを、観測値と理論値の差（残差）を用いて評価する統計量。値が小さいほど当てはまりが良い。`},
{target:`層化抽出`, content:`母集団の性質を標本に投影させる方法。<br>各層の割合を保ちながらランダムにサンプリングすることで、標本が母集団をより適切に代表する。StratifiedShuffleSplitクラス。`},
{target:`fitメソッド`, content:`推定器。データセットに基づいてパラメータを推計するメソッド。<br>推定できるオブジェクトは推定器（estimator）と呼ばれる。ハイパーパラメータは、インスタンス変数として設定する（またはコンストラクタへの引数として渡す）。`},
{target:`transformメソッド`, content:`変換器。<br>引数として変換対象のデータセットを受け取り、戻り値として変換後のデータセットを返すメソッド。変換は、学習済みのパラメータを使用して行われる。`},
{target:`fit transformメソッド`, content:`fit（）を呼び出してからtransform（）を呼び出す一連の処理をまとめたメソッド。`},
{target:`predictメソッド`, content:`予測器。<br>データセットを与えることで予測を行うメソッド。`},
{target:`predict probaメソッド`, content:`モデルが予測する各クラスの確率を返すメソッド。`},
{target:`インスペクション`, content:`推定器のハイパーパラメータは、公開されたインスタンス変数を通じて直接アクセスでき、学習後のパラメータも公開されたインスタンス変数を介してアクセスできる（imputer.statistics_など）。`},
{target:`ワンホットエンコーディング`, content:`カテゴリごとに1個のバイナリ属性を作るための変換手法。<br>OneHotEncoderクラス。`},
{target:`正規化`, content:`データの値を0から1の範囲に変換する手法。<br>データの値域や尺度を一定の基準に従って変換することで、データの比較や分析を容易にするための手法。MinMaxScalerクラス。`},
{target:`標準化`, content:`データの平均を0に、分散（標準偏差も）を1にする手法。<br>StandardScalerクラス。`},
{target:`K分割交差検証`, content:`データをK個に分割し、そのうち1つをテストデータ、残りを学習データとして使い、K回繰り返して正解率などを評価する手法。<br>cross_val_scoreクラス。`},
{target:`グリッドサーチ`, content:`最適なハイパーパラメータ値の組み合わせを見つける手法。<br>GridSearchCVクラス。`},
{target:`ランダムサーチ`, content:`個々のハイパーパラメータのために無作為に値を選び、それらを組み合わせて評価する手法。<br>RandomizedSearchCVクラス。`},
{target:`確率的勾配降下法 SGD`, content:`勾配法の最適化手法で、必要な勾配の計算を確率を用いて高速に行う方法。<br>各ステップで訓練セットから無作為に1つのインスタンスを選び、そのインスタンスだけを使って勾配を計算する手法。大規模なデータセットを効率よく扱える。SGDClassifierなど。`},
{target:`多クラス分類器`, content:`2つ以上のクラスを見分ける分類器。<br>OVA法（One-Vs-All法）の方が良いとされており、SVC（）クラスは自動的にOVA法とOVO法を使い分ける。`},
{target:`OVA法`, content:`個々の「二項分類器」を訓練し、各分類器の決定スコアを比較し、もっとも高いスコアを出力した分類器のクラスを選ぶ手法。`},
{target:`OVO法`, content:`すべてのクラスのペアに対して「二項分類器」を訓練し、すべての分類器を実行して、もっとも多くの勝利を収めたクラスを選ぶ手法。`},
{target:`多ラベル分類`, content:`複数の2値タグを出力する分類システム。<br>KNeighborsClassifierクラスなど。`},
{target:`RMSE`, content:`二乗平均平方根誤差。`},
{target:`MSE`, content:`平均二乗誤差。`},
{target:`勾配降下法 GD`, content:`訓練セットに対して損失関数が最小になるように、モデルパラメータを少しずつ操作し、収束する手法。<br>学習率を用いてパラメータを更新する。`},
{target:`学習率`, content:`勾配降下法のステップのサイズ。<br>値小さすぎると収束までの反復数が増え、大きすぎると発散して最小値に収束できない。`},
{target:`バッチ勾配降下法`, content:`勾配降下ステップごとに、訓練データ全体のバッチを使う手法。<br>大規模なデータセットだと遅くなる。`},
{target:`イテレーション`, content:`配列などに対する反復処理。<br>一連の工程を短期間で繰り返す、開発サイクル。`},
{target:`焼きまなし法`, content:`最小値に収束しないジレンマを解決するため、「学習率」を少しずつ小さくする手法。`},
{target:`学習スケジュール`, content:`各イテレーションの学習率を決める関数。<br>学習率やミニバッチサイズなどのハイパーパラメータをステップごとに変化させる手法。学習率の調整が適切でないと、最適な解に収束できない可能性がある。`},
{target:`エポック`, content:`m回のイテレーションを1ラウンドとしたときの各ラウンド。<br>個々のパスで訓練セット全体を処理する回数。`},
{target:`ミニバッチ勾配降下法`, content:`ミニバッチと呼ばれる無作為に選ばれたインスタンスの小さな集合を使って勾配を計算する手法。`},
{target:`学習曲線`, content:`訓練セットのサイズに対する訓練セットや検証セットに対する性能をプロットしたもの。`},
{target:`データエラー`, content:`データの誤り、欠落、または損失。<br>入力エラー、出力エラー、転送エラー、処理エラーなど。`},
{target:`バイアス`, content:`前提条件のまずさに起因（2次データを線形と考えるなど）。<br>データの収集やサンプリングの方法による。これが高いモデルは、訓練データに対して過小適合しやすい。モデルのパフォーマンスや予測の信頼性に影響を与える。値が大きい場合、モデルの予測は真の値からのずれや誤った方向性を示す。適切に理解し、可能な限り低減するための改善や補正が重要。低減には、適切な特徴の選択や追加、モデルの複雑性の調整、サンプリングのバランスの取り方など、さまざまなアプローチが必要。`},
{target:`分散`, content:`モデルが訓練データの小さな差異に敏感すぎることに起因する。<br>この値が大きい場合、モデルは訓練データに過度に適合し、新しいデータに対して汎化性能が低くなる。`},
{target:`削減不能誤差`, content:`データ自体のノイズに起因する誤差。<br>この部分の誤差を減らす唯一の方法は、データのクリーンアップ。`},
{target:`ノイズ`, content:`データや信号での不要な情報や乱れ。<br>データの収集や伝送の過程で発生、データの品質や信号のクオリティを損なう可能性がある。これが大きい場合、データの特徴やパターンを正確に把握することが難しくなり、データ分析や機械学習モデルのパフォーマンスに悪影響を与える。この除去や軽減にはフィルタリング、平滑化、除去アルゴリズムの適用、信号の再サンプリングなど。またこの発生源を特定し、それに対する適切な対策を講じることも重要。データ分析や機械学習のモデル設計において、これに強い特徴抽出やモデルの選択、アンサンブル学習などの手法を使用することも有効。`},
{target:`Lasso回帰`, content:`L1正則化を使用した回帰。<br>1次の正則化項を導入し、不要な特徴の係数を0に推進する傾向がある。Lassoクラス。`},
{target:`Ridge回帰`, content:`L2正則化を使用した回帰。<br>2次の正則化項を導入し、各特徴の係数が0になりにくくなる。Ridgeクラス。`},
{target:`ElasticNet`, content:`Ridge回帰とLasso回帰の中間の正則化手法。<br>L1正則化とL2正則化の両方を組み合わせて使用する。ElasticNetクラス。`},
{target:`早期打ち切り`, content:`検証誤差 （RMSE） が最小になった時点で訓練を中止する。<br>単純でありながら、効率的な正則化テクニックであり、素晴らしいフリーランチである。「EarlyStoppingコールバック」で実装できる。patience引数で、指定された「エポック数」だけ「検証セット」に対する性能が上がらないときに、訓練を中止し、オプションで最良のモデルにロールバックする。`},
{target:`ロジステック回帰`, content:`インスタンスが特定のクラスに属する確率を推定する。<br>0から1までの値を出力するシグモイド関数を使用。LogisticRegression（）クラス。`},
{target:`ソフトマックス回帰`, content:`インスタンスxを受け取り、各クラスkのために sk（x） を計算し、この関数を適用して各クラスの確率を推定する。<br>推定された確率が最も高いクラスを予測として返す。`},
{target:`ランダムフォレスト`, content:`分類や回帰問題に使用されるアルゴリズム。<br>データセットからランダムにサンプリングを行い、そのサンプルから決定木を作成。各決定木にはランダムに選択された特徴量が利用され、複数の決定木が出力した結果を集約して最終的な予測を行う。`},
{target:`サポートベクターマシン`, content:`分類や回帰問題に使用される教師あり学習アルゴリズム。<br>2つのクラスの境界を定義する最適な超平面を見つけることによってデータを分類。非線形分類器の場合、カーネルトリックを使用して高次元の特徴空間でデータを分類。SVCクラス。`},
{target:`ハードマージン分類`, content:`マージンの内側に訓練データが存在しないように、クラスを分離する最適な線形境界を見つける。`},
{target:`ソフトマージン分類`, content:`マージンの内側に誤分類されたデータを許容し、誤分類されたデータ点に罰則を与える。<br>Cが小さいほど、誤分類を許容する範囲が広くなる。`},
{target:`カーネルトリック`, content:`特徴ベクトルを非線形変換して、その空間で線形の識別を行う手法。<br>高次元の特徴空間において、線形分離不可能なデータを扱うことができる。`},
{target:`アンサンブル`, content:`一群の予測器の予測を1つにまとめる方法。`},
{target:`バギング`, content:`サンプリングを重複ありで行う手法。`},
{target:`ペースティング`, content:`サンプリングを重複無しで行う手法。`},
{target:`OOB Out of bag`, content:`バギングでサンプリングされないインスタンス。<br>データの37%（予測器ごとに異なる）。OOBインスタンスを使って検証することをOOB検証と呼ぶ。個々の予測器のOOB検証を平均すると、アンサンブル自体の検証結果になる。`},
{target:`ブースディング`, content:`最初の弱い学習器をトレーニングし、その結果を評価。次に、誤分類されたデータに重みを付けて再トレーニングし、次の弱い学習器を作成。<br>このプロセスを繰り返し、最終的に、すべての学習器の出力を重み付き平均することで最終的な予測を行う。`},
{target:`アダブースト`, content:`ブースディングの一つ。<br>訓練データセットで正しく分類されなかったデータに重みを与え、再度別の弱い学習器を構築する際、前回学習されたモデルの誤差に応じて、重みが調整される。`},
{target:`勾配ブースディング`, content:`ブースディングの一つ。<br>誤差関数の勾配情報を使って、前回のモデルの残差を学習するように新たなモデルを作成。誤分類されたデータに重点的に学習し、モデルの性能を向上。過学習を防ぐために、学習率や正則化項を設定することができる。`},
{target:`勾配ブースディング回帰木 GBRT`, content:`勾配ブースディングを使った回帰。`},
{target:`XGBoost`, content:`勾配ブースティングの一種。分割の際に最適な特徴量と閾値を見つけるために、勾配降下法を使用。<br>また、過学習を防ぐために正則化項を追加できる。高速で高い精度を発揮し、特徴量の重要度を算出できるため、様々な機械学習タスクで有用。`},
{target:`バイナリ入力`, content:`0と1の二進数で表される情報を入力として扱うこと。`},
{target:`ANN artificial neural networks`, content:`人工知能。`},
{target:`TLU threshold logic unit`, content:`閾値論理素子。`},
{target:`φ`, content:`活性化関数。`},
{target:`X`, content:`入力特徴量の行列を表す。<br>インスタンスごとに1行、特徴量ごとに1列が使われる。`},
{target:`W`, content:`重み行列。`},
{target:`b`, content:`バイアスベクトル。`},
{target:`η`, content:`イータ、学習率。`},
{target:`順伝播型ニューラルネットワーク FNN feed forward neural network`, content:`一方向に信号が伝播する多層パーセプトロン（Multilayer Perceptron）の一種で、最も基本的なニューラルネットワーク。<br>一般的には、入力層、中間層（隠れ層）、出力層の3つの層から構成。`},
{target:`パーセプトロン perceptron`, content:`「ANNアーキテクチャ」でもっとも単純なものの1つ。<br>「TLU」（threshold logic unit 「閾値論理素子」）と呼ばれるわずかに異なる「人工ニューロン」を基礎とし、入出力は数値（x）で、個々の入力の接続部には「重み」（w 接続重み）が与えられる。そして、「TLU」は、入力の加重総和（z = xw）を計算し、その総和に「ステップ関数」（step（z） step function）を適用し、結果（hw（x）=step（xw））を出力する。`},
{target:`全結合層 fully connected layerもしくは密層 dense layer`, content:`ある層の全ての「ニューロン」が前の層のすべての「ニューロン」に接続される。`},
{target:`生物学的ニューロン`, content:`「ニューロン」がほかの「ニューロン」を発火するうちに、両者の間のつながりが強化されるとして、「パーセプトロン」は、1度に1つの訓練インスタンスを与えられ、個々のインスタンスに対して予測を行う。<br>正しい予測を生み出すために役立ったはずの「入力ニューロン」の「接続の重み」をあげる。`},
{target:`MLP Multi Layer Perceptron`, content:`1つの「入力層」と「隠れ層」（hidden layer）と呼ばれる1つ以上の「TLU層」、「出力層」（output layer）と呼ばれる最後の1つの「TLU層」から構成される。<br>「入力層」に近い層は「下位層」（lower layer）、「出力層」に近い層は「上位層」（upper layer）と呼ばれる。「出力層」を除く各層にば「バイアスニューロン」が含まれており、次の層と全結合されている。`},
{target:`深層ニューラルネットワーク Deep Neural Network DNN`, content:`「ANN」が深い層を保つ場合。`},
{target:`バックプロパゲーション back propagation 誤差逆伝播法`, content:`勾配を自動計算する効率的なテクニックを使った「勾配降下法」であり、誤差を削減するために個々の「接続重み」と個々の「バイアス項」をどのように操作すべきか自動的に明らかにする。<br>勾配が得られたら、通常の「勾配降下法」を実行し、ネットワークが解を見つけて収束するまでこのプロセスを繰り返す。勾配の自動計算は、「自動微分」（automatic differentiation）と呼ばれる。`},
{target:`勾配降下アルゴリズム`, content:`個々の訓練インスタンスに対して、まず予測を行い（「前進パス」）、誤差を計算してから、各層を逆戻りしながら個々の「接続部」が誤差にどの程度の影響を与えたかを計測し（「後退パス」）、最後に誤差が小さくなるように「接続重み」を調整する（「勾配降下ステップ」）。<br>すべての「隠れ層」の「接続重み」を無作為に初期化することが大切であり、そうしないと訓練が失敗する。重みを無作為に初期化すれば、対称性が破られ、「バックプロパゲーション」が「ニューロン」を多様な形で訓練できる。`},
{target:`ロジステック関数シグモイド関数`, content:`入力値を0から1の間にスケーリングし、確率的な意味を持たせることができる。<br>シグモイド関数は、その形状がS字型に近く、連続的に微分可能な特徴を持つため、多層パーセプトロンにおいて広く使用されている。また、シグモイド関数は、中間層において非常に大きな値をとる場合に、勾配消失問題が発生することがある。`},
{target:`双曲線正接 hyperbolict angent`, content:`「ロジスティック関数」と同様に、S字型で連続で微分可能だが、出力は-1から1までの範囲になる。`},
{target:`ReLU関数`, content:`f（x）=max（0,x）。<br>連続関数だが z=0で微分可能でなく、z=0での導関数が0になってしまう。しかし、実際に使ってみると、勾配消失問題が発生しにくく、よく機能し、短時間で計算できるというメリットがあるためデフォルトになっている。`},
{target:`ソフトプラス活性化関数 softplus`, content:`ReLU関数の滑らかな近似関数。<br>ReLU関数と同様に、入力値が正の場合は出力値が増加するという特徴がある。ただし、ReLU関数とは異なり、入力値が負の場合にも出力値が滑らかに増加するため、デッドニューロン問題が発生することがない。主にニューラルネットワークにおいて、中間層に使用されることがある。また、多クラス分類問題において、出力層においてソフトマックス関数と組み合わせて使用することがある。`},
{target:`フーバー損失関数`, content:`誤差が閾値σ（一般に1）よりも小さければ二乗、σよりも大きければ線形になる。<br>線形部は、平均二乗誤差よりも外れ値に敏感にならず、二乗部は平均絶対誤差よりも早く収束し正確である。`},
{target:`回帰の入力ニューロン数`, content:`入力特徴量あたり1個。`},
{target:`回帰の隠れ層の数`, content:`一般的に、1～5。`},
{target:`回帰の隠れ層ごとのニューロン数`, content:`一般的に、10～100。`},
{target:`回帰の出力ニューロン`, content:`予測次元あたり1個。`},
{target:`回帰の隠れ層の活性化関数`, content:`「ReLU」（もしくは「SELU」）。`},
{target:`回帰の出力層の活性化関数`, content:`「ReLU」/「ソフトプラス」（出力を正にしたい場合）、「ロジステック」/「双曲線正接」（出力の範囲が決まっている場合）。`},
{target:`回帰の損失コスト関数`, content:`「平均二乗誤差」か、「平均絶対誤差」、「フーバー」（「外れ値」が多い場合）。`},
{target:`ソフトマックス関数`, content:`入力値を確率分布として扱い、それぞれの要素が各クラスに属する確率を表す。<br>多クラス分類問題において出力層で使用される。この場合、出力層のノード数はクラスの数と一致し、各ノードが各クラスに属する確率を表す。すべての「クラス」の「推定確率」が0から1までの値になり、それらを合計すると1になる。`},
{target:`分類の入力ニューロン数`, content:`二項…入力特徴量あたり1個、多ラベル二項…入力特徴量あたり1個、多クラス…入力特徴量あたり1個。`},
{target:`分類の隠れ層の数`, content:`二項…一般的に1～5。<br>多ラベル二項…一般的に1～5。<br>多クラス…一般的に1～5。`},
{target:`分類の隠れ層ごとのニューロン数`, content:`二項…一般的に10～100。<br>多ラベル二項…一般的に10～100。<br>多クラス…一般的に10～100。`},
{target:`分類の出力ニューロン`, content:`二項…1個。<br>多ラベル二項…ラベルごとに1個。<br>多クラス…クラスごとに1個。`},
{target:`分類の隠れ層の活性化関数`, content:`二項…ReLU（もしくはSELU）。<br>多ラベル二項…ReLU（もしくはSELU）。<br>多クラス…ReLU（もしくはSELU）。`},
{target:`分類の出力層の活性化関数`, content:`二項…ロジステック。<br>多ラベル二項…ロジステック。<br>多クラス…ソフトマックス。`},
{target:`分類の損失コスト関数`, content:`二項…交差エントロピー。<br>多ラベル二項…交差エントロピー。<br>多クラス…交差エントロピー。`},
{target:`損失関数 loss`, content:`モデルが算出した予測値と、実際の正解値のズレを計算するための関数。`},
{target:`最適化 optimizer`, content:`与えられた制約条件下で目的関数を最小化または最大化するために、パラメータや変数の値を適切に調整する手法。`},
{target:`評価指標 metrics`, content:`モデルの予測結果と真の値との間の誤差を計算すること。<br>モデルの予測精度を評価するための指標。`},
{target:`汎化誤差`, content:`訓練データセットではなく未知のデータに対して、どれくらい正確に予測できるかを示す指標。`},
{target:`初期化子 initializer`, content:`変数を初期化するために使用される値や式。`},
{target:`正則化器 regularizer`, content:`過学習を防ぐために使用される手法の一つ。<br>モデルの損失関数に正則化項を加えることで、モデルの複雑さを抑える。`},
{target:`重み weight`, content:`入力特徴量と出力の関係を定義するパラメータの一つ。<br>ニューラルネットワークを含む多くの機械学習モデルでは、各層において入力と出力を関連付けるために、重み行列を使用。具体的には、入力データを行列として表現し、重み行列との行列積を取ることで、各層の出力を計算。`},
{target:`デプロイ`, content:`開発したソフトウェアを実際の運用環境に配置・展開して実用に供すること。`},
{target:`ワイド アンド ディープニューラルネットワーク`, content:`「ディープパターン」（ディープパス）と「単純ルール」（ワイドパス）の両方を学習する「ニューラルネットワーク」。<br>入力層の1つには、浅い層（Wide）を配置し、他方の入力層には、深い層（Deep）を配置する。Wide層は、一部の入力特徴量に強く反応し、より広い特徴空間をカバーするように設計されている。一方、Deep層は、高次元の特徴表現を学習し、複雑な関係性を捉えることができるように、深い隠れ層を持つように設計されている。推薦システム、検索エンジン、広告配信などのビジネス分野で高い精度を発揮している。特に、スパースなデータを含む場合に優れた性能を示す。`},
{target:`コールバック`, content:`fit（）メソッドで「チェックポイント」の保存を指示する。<br>callback引数を持っており、訓練の開始、終了時、「各エポック」の開始、終了時、さらに、「バッチ」を1つ処理する前後に Kerasが呼び出すオブジェクトのリストを指定できる。「ModelCheckpointコールバック」は、訓練中、定期的な感覚でモデルの「チェックポイント」を保存する。「ModelCheckpointコールバック」を作るとき、save_best_only=Trueを設定できる。これにより、最も良いモデルだけを保存できる。これを使えば、「過学習」を気にしなくて済む。`},
{target:`Hyperopt`, content:`ハイパーパラメータの最適化に使えるPythonライブラリ。<br>あらゆるタイプの複雑な「探索空間」を「最適化」する広くつかわれているライブラリ。`},
{target:`Hyperas kopt Talos`, content:`ハイパーパラメータの最適化に使えるPythonライブラリ。<br>Kerasモデルのハイパーパラメータを最適化する便利なライブラリ。`},
{target:`KerasTuner`, content:`ハイパーパラメータの最適化に使えるPythonライブラリ。<br>Googleによる Kerasモデル用の使いやすいハイパーパラメータ最適化ライブラリ。`},
{target:`Scikit Optimize`, content:`ハイパーパラメータの最適化に使えるPythonライブラリ。<br>汎用の「最適化ライブラリ」。BayseSearchCVクラスは、GridSearchCVと似たインターフェイスを使って「ベイズ最適化」を行う。`},
{target:`Spearmint`, content:`ハイパーパラメータの最適化に使えるPythonライブラリ。<br>「ベイズ最適化ライブラリ」。`},
{target:`Hyperband`, content:`ハイパーパラメータの最適化に使えるPythonライブラリ。<br>高速な「ハイパーパラメータ調整ライブラリ」。`},
{target:`勾配消失問題`, content:`深いニューラルネットワークを学習する際に、逆伝搬による勾配の値が小さくなっていくため、下位層に逆伝搬された勾配がほとんど更新されなくなり、学習が進まなくなる現象。`},
{target:`バッチ正規化`, content:`ニューラルネットワークの中間層の出力を正規化すること。<br>学習を安定化させ、高速化する手法の一つ。`},
{target:`勾配クリッピング`, content:`ニューラルネットワークの学習において、勾配があまりにも大きい場合に、勾配の大きさを制限する手法。`},
{target:`勾配`, content:`各パラメータに対する損失関数の勾配を表す。<br>誤差逆伝搬法（backpropagation）において計算され、重みやバイアスなどのパラメータの更新に使用。学習の目的は、損失関数を最小化するようにこれらのパラメータを調整すること。この調整は、勾配を計算して勾配降下法などの最適化アルゴリズムを用いて行われる。`},
{target:`転移学習`, content:`あるタスクで学習されたモデルの一部や全体を、異なるタスクの学習に再利用する手法。`},
{target:`自己学習`, content:`未ラベルのデータを使用して、モデルを自己で学習させる手法。<br>未ラベルのデータを入力としてモデルを学習し、その出力を一部のラベルとして扱い、再度学習を繰り返す。`},
{target:`オプティマイザ`, content:`機械学習において、学習アルゴリズムが探索する最適なモデルのパラメータを自動的に最適化するためのアルゴリズム。<br>損失関数を最小化するようにモデルのパラメータの調整を自動的に行うためのアルゴリズムであり、勾配降下法などの最適化手法が代表的。`},
{target:`モーメンタム最適化`, content:`勾配降下法において、勾配に基づく更新に加え、過去の勾配の影響を加えることで、より滑らかな更新を実現する手法。`},
{target:`正則化`, content:`過学習を抑制するため、損失関数に加えることで、重みの値を小さくしたり、モデルの複雑度を抑える。`},
{target:`リファクタリング`, content:`既存のコードを改善し、保守性や可読性、拡張性を向上させるプロセス。`},
{target:`ドロップアウト`, content:`正則化手法の一つで、学習中にランダムに一部のニューロンを無効化することによって過学習を防止する手法。`},
{target:`重み上限正則化`, content:`重みの値をある閾値以下に制限することで、過学習を抑制する。`},
{target:`自動グラフ生成機能 Automatic Graphing Function`, content:`データ分析や可視化において、プログラムからグラフを自動生成する機能。<br>この機能は、機械学習やデータ分析のような複雑なデータ処理を行う際に、多くの情報を視覚的に理解しやすくする。`},
{target:`計算グラフ computational graph`, content:`数式をノードとエッジを持つ有向グラフとして表現。<br>ノードは数式の演算を表し、エッジはノード間の入出力関係を表す。各ノードは、入力値を受け取り、演算を行った結果を出力する。ノードの出力は、次のノードの入力になる。`},
{target:`API`, content:`コンピュータプログラム（ソフトウェア）の機能や管理するデータなどを、外部の他のプログラムから呼び出して利用するための手順やデータ形式などを定めた規約。`},
{target:`低水準API`, content:`コンピューターシステムの内部機能にアクセスするためのプログラミングインターフェース。`},
{target:`高水準API`, content:`より抽象的なレベルの機能を提供し、より簡単に使用できるように設計されたプログラミングインターフェース。`},
{target:`カーネル`, content:`オペレーティングシステム（OS）の基本機能の役割を担うソフトウェア。`},
{target:`チャンク`, content:`大きなデータを分割して制御情報を付加したひとまとまりの断片。`},
{target:`テンソル tensor`, content:`複数の次元を持つ配列。`},
{target:`疎テンソル sparse tensor`, content:`テンソルの中で、ほとんどの要素がゼロであるような疎な構造を持つテンソル。`},
{target:`シーケンス`, content:`順番に並んだ一続きのデータや手順のことや、並んだ順番にデータや手順を取り扱う処理方式などのこと。`},
{target:`フェッチ`, content:`データを取りにいくこと。`},
{target:`プリフェッチ`, content:`この先必要になるデータなどを予め読み込んでおき、読み込みにかかる時間を短縮する仕組み。`},
{target:`アーキテクチャ`, content:`基本設計や共通仕様、設計思想。`},
{target:`コンポーネント`, content:`システムやソフトウェアの一部を構成する独立した要素や部品。<br>特定の機能や機能セットを実現するために設計され、独自のインターフェースを持つ。これにより、これは他のと連携し、大規模なシステムやアプリケーションを構築するために組み合わせることができる。他のプログラムから呼び出されたり連結されたりして使用されるプログラム部品のこと。`},
{target:`アンロール`, content:`展開。`},
{target:`バディング`, content:`データを一定の長さに整形するため前後に挿入される無意味なデータ。`},
{target:`トークン`, content:`長いデータを最小の構成単位に分解したもの。`},
{target:`リポジトリ`, content:`プログラムのソースコードやドキュメント、関連する各種のデータやファイルなどを一元的に管理する格納場所。`},
{target:`パース`, content:`一定の書式や文法に従って記述されたデータを解析し、プログラムで扱えるようなデータ構造の集合体に変換する。`},
{target:`バケット`, content:`データをある長さごとに区切り。`},
{target:`スタック`, content:`データ構造の一つ。要素が入ってきた順に一列に並べ、後に入れた要素から順に取り出すという規則で出し入れを行う。`},
{target:`キャスト`, content:`あるデータ型を別のデータ型に変換する操作。`},
{target:`ラップ`, content:`あるクラスや関数、データ型などが提供する機能やデータを含み、別の形で提供すること。`},
{target:`ラッパー`, content:`そのようにして用意されたクラスや関数などのこと。<br>元の機能を包み、覆い隠す役割を果たす。`},
{target:`インフラストラクチャ`, content:`情報システムを稼動させる基盤、土台、下部構造。`},
{target:`ハードウェアアクセラレータ`, content:`機器やソフトウェア、システムなどに追加し性能を向上させる機材。`},
{target:`インジェスト`, content:`データを作業用のストレージにコピー・移動する。`},
{target:`バンドリング`, content:`組み合わせる。`},
{target:`サーブ`, content:`提供する。`},
{target:`キューイング`, content:`「キュー」を用いて要素の管理を行うこと。`},
{target:`キュー`, content:`データを先入れ先出しのリスト構造で保持するもの。`},
{target:`シリアライズ`, content:`複数の要素を一列に並べる操作や処理。<br>プログラムの実行状態や複雑なデータ構造などを一つの文字列やバイト列で表現する「直列化」を指す。`},
{target:`スライス slice`, content:`Pythonのリストや文字列、配列などのシーケンス型オブジェクトの一部分を取り出すための機能。`},
{target:`メタグラフ`, content:`計算グラフに関数シグネチャ定義（入出力の名前、方、形）を加えたもの。`},
{target:`バッファ`, content:`データを一時的に格納するためのメモリー領域。`},
{target:`レコード`, content:`「データベース」内のテーブルを構成する単位のひとつで、一行分のデータをさす。`},
{target:`インターリープ`, content:`データ入出力で連続的に信号やデータを扱う際、時間や空間などの何らかの物理的な広がりに対してわざと不連続にデータを配置する手法。`},
{target:`キャッシュ`, content:`直近に読み込んだデータや使用頻度が高いデータを高速な記憶装置に複製しておくこと。`},
{target:`TFRecord形式`, content:`TensorFlowが提供するデータセットフォーマットの一つ。<br>バイナリフォーマットのデータをシリアル化して保存するための形式。この形式のデータは、大量のデータを効率的に読み込むことができ、TensorFlowを使った機械学習のモデルトレーニングに適している。`},
{target:`シリアル化 serialization`, content:`あるデータのオブジェクトをバイト列などの連続したデータに変換する処理のこと。`},
{target:`プロトコルバッファ protocol buffers`, content:`Googleが開発したデータシリアル化のためのプラットフォーム。<br>拡張性の高いオープンソース形式。構造化データをバイト列に変換（シリアライズ・シリアル化）するための仕組み。異なるプログラミング言語やプラットフォーム間でデータのやり取りをする際に利用。`},
{target:`コンパイル`, content:`プログラミング言語で書かれたコンピュータプログラム（ソースコード）を解析し、コンピュータが直接実行可能な形式のプログラム（オブジェクトコード）に変換すること。`},
{target:`エンコード`, content:`ある形式から別の形式に変換すること。<br>一般的に、データをコンピュータが処理しやすい形式に変換すること。`},
{target:`Lambda層`, content:`KerasやTensorFlowなどのディープラーニングフレームワークで提供される機能の一つ。<br>自分で関数を定義してニューラルネットワークのモデルに組み込むことができる層。`},
{target:`ワンホットエンコーディング one hot encoding`, content:`カテゴリ変数を数値変数に変換するための一般的な方法の一つ。<br>カテゴリ変数の値を0または1の値を持つバイナリ変数に変換する手法。`},
{target:`埋め込み embedding`, content:`高次元の離散的なデータを、より低次元の密な連続的なベクトルに変換する手法。`},
{target:`畳み込みニューラルネットワーク Convolutional Neural Network CNN`, content:`主に画像認識などのタスクに用いられる、深層学習モデルの一種。入力データに対して畳み込み演算を行うことで、特徴量を抽出する。<br>畳み込み演算は、入力データにフィルターを適用し、局所的な特徴を抽出する手法、画像認識では、エッジや角、ブロブなどの特徴を抽出する。CNNは、畳み込み層とプーリング層（サブサンプリング層）を交互に配置して構成される。プーリング層は、畳み込み層で抽出された特徴量をさらに圧縮する役割を持つ。最後に、全結合層を用いて、プーリング層で抽出された特徴量を元に、入力データの分類や予測を行う。全結合層は、入力層と出力層の全てのニューロンが互いに接続されており、それぞれの結合に重みが割り当てられる。`},
{target:`畳み込み層 convolutional layer`, content:`CNNにおいて、画像や音声などのデータから特徴を抽出するために用いられる層の一つ。<br>この層では、フィルター（カーネル）と呼ばれる小さな行列を使って、入力データに対して演算を行う。この演算は、フィルターを移動させながら入力データとの積和演算を行うことで、特徴量を抽出する操作。画像を入力データとしてこの層に与えた場合、この層は画像内のエッジやブロブなどの特徴を抽出する。この層では、複数のフィルターを用いることで、異なる種類の特徴を抽出することができる。この層では、フィルターのサイズ、ストライド（フィルターを移動させる距離）、パディング（入力データの周囲に0を埋めること）などのパラメータを調整することで、この層が抽出する特徴の大きさや種類を調整することができる。`},
{target:`プーリング層 pooling layer`, content:`CNNにおいて、畳み込み層で抽出された特徴マップを縮小するために用いられる層の一つ。<br>この層では、畳み込み層の出力に対して、ある領域内の最大値や平均値を取ることで、特徴マップを縮小する。これにより、特徴マップのサイズを小さくし、計算量を減らすことができる。また、この層によって、特徴マップ内の微小な位置の違いに対して、モデルがより頑健になる効果がある。一般的には、最大値プーリング （Max pooling） や平均値プーリング （Average pooling） が使用される。最大値プーリングでは、領域内の最大値を取り、平均値プーリングでは、領域内の平均値を取る。この層においても、フィルターのサイズやストライドなどのパラメータを調整することで、特徴マップをどの程度縮小するかを調整することができる。この層は、畳み込み層と交互に重ねて用いることで特徴マップを階層的に縮小し、より高度な特徴の抽出が可能になる。`},
{target:`受容野 receptive field`, content:`畳み込みニューラルネットワークにおいて、ある特徴マップ上の1つの出力値に影響を与える入力領域のこと。<br>畳み込み層を通過することでどれだけの入力領域が出力に寄与するかを示す。`},
{target:`ゼロパディング zero padding`, content:`畳み込みニューラルネットワークにおいて、入力データの周囲に0を埋めることで、出力サイズを調整する手法。<br>これにより、畳み込み演算を適用する際に端の特徴も適切に捉えることができる。`},
{target:`ストライド stride`, content:`畳み込みニューラルネットワークにおいて、フィルターを適用する際にフィルターの移動量を表すパラメータ。<br>この値を変更することで、畳み込み演算におけるフィルターの適用範囲を調整できる。これが大きくなるほど、フィルターが適用される範囲が小さくなり、出力の空間解像度が下がる。`},
{target:`フィルター filter または畳み込みカーネルconvolutional kernel`, content:`畳み込みニューラルネットワークにおいて、入力データに対して畳み込み演算を行うために使用される重み行列のこと。<br>この値は、初期化によってランダムに設定され、学習によって調整される。このサイズやストライドなどのハイパーパラメータは、ネットワークの設計において重要な役割を果たす。`},
{target:`特徴量マップ feature map`, content:`畳み込みニューラルネットワークにおいて、畳み込み層やプーリング層の出力として得られる画像のような3次元配列。<br>畳み込み演算では、フィルターが入力データ上をスライドしながら、各位置における重み付き和を計算。この重み付き和を計算した結果が、出力として得られるマップになる。それぞれのフィルターによって得られたマップは、異なる種類の特徴を表現している。プーリング層では、このマップを縮小することで、特徴の位置に対するロバスト性を向上させる。このマップのすべてのニューロンが同じパラメータを共有するため、モデル内のパラメータの数は劇的に削減される。バイアス項はこのマップの全体の明るさを調整するためのノブと考えることができる。`},
{target:`変換不変 translation invariant`, content:`データの変換（translation）に対して不変であること。<br>画像を少し平行移動させたり、回転させたり、反転させたりした場合でも、同じ物体であると認識できる性質。`},
{target:`グローバル平均プーリング層 global average pooling layer`, content:`特徴量マップを平均化して1つの値に変換する層の一種。<br>畳み込みニューラルネットワークの最後の層として用いられることがある。出力層における計算コストを低減するとともに、過学習を抑制する正則化の効果がある。`},
{target:`スタックトップ`, content:`スタックの一番上のデータがある位置。`},
{target:`局所応答正規化層 Local Response Normalization LRN`, content:`ニューラルネットワークの畳み込み層の後に適用される正則化手法の一つ。<br>畳み込み層の出力に対して、同じチャンネル内の近隣の出力に対して正規化する。つまり、同じチャンネル内の各ピクセルについて、その近傍の値の二乗和に対して正規化することで、そのピクセルの特徴量の強度を調整する。`},
{target:`GoogLeNet`, content:`Googleが2014年に開発したディープラーニングの畳み込みニューラルネットワーク（CNN）。<br>畳み込み層、プーリング層、そしてインセプションモジュールと呼ばれる特別な構成のモジュールで構成。`},
{target:`インセプションモジュール`, content:`1x1、3x3、5x5の畳み込み層と、1x1の畳み込み層による次元削減を同時に行い、効率的な計算を行うことができる。<br>また、畳み込み層の出力を直接結合し、ネットワークの構造を浅く、広くできる。畳み込み層のフィルターサイズやストライド、パディングの調整、次元削減の割合などを調整することで、ネットワーク全体の性能を最適化することができる。また、深いネットワークを構築する場合にも有効であり、最新のディープラーニングモデルでも広く採用されている。`},
{target:`残差ネットワーク Residual Network ResNet`, content:`2015年にMicrosoft Researchが提唱したディープラーニングのネットワーク構造の一つ。<br>従来の深いニューラルネットワークでは勾配消失問題が発生し、学習が難しいという問題を、スキップ接続を導入することで解決した。スキップ接続とは、入力を出力に直接加算することで、情報の損失を抑えることができる。`},
{target:`スキップ接続 skip connection`, content:`ニューラルネットワークの層間において、一部の入力が出力に直接加算する。<br>これにより層の深さが増えた際に生じる勾配消失問題を緩和する。多数のこの接続を追加すると、ネットワークはいくつかの層がまだ学習を始めてなくても、先に進めるようになり、信号はネットワーク全体に簡単に行き渡るようになる。`},
{target:`Xception`, content:`GoogLeNetのバリアントで、大規模なビジョンタスク（3.5億の画像と1.7万のクラス）で高い性能を示す。<br>「GoogLeNet」と「ResNet」のアイデアを結合したもの。インセプションモジュールの代わりに深層分離可能な畳み込み層（depthwise separable convolution layer、または、分離可能畳み込み層 separable convolution layer）を使っている。通常の畳み込み層は空間パターン（たとえば楕円などの図形）と交差チャネルパターン（たとえば口+鼻+目=顔）を同時に捉えるためにフィルタを使う。分離可能畳み込み層は空間パターンと交差チャネルパターンを別々にモデリングできることを前提としている。分離可能畳み込み層を主要な演算として用い、畳み込み層の効率的なスタックによって、非常に高い精度を持つ画像認識タスクのモデルを実現。`},
{target:`分離可能畳み込み層 separable convolution layer`, content:`畳み込み演算を2つのステップに分割した畳み込み層のこと。<br>この層では、空間的な畳み込みと深度方向の畳み込みを個別に適用する。まず、空間的な畳み込みにより、入力データとカーネルとの積和演算が行われ、畳み込み特徴マップを生成。次に、畳み込み特徴マップの各ピクセルに対して、深度方向の畳み込みが行われ、最終的な出力を得る。このように、この畳み込み層は、畳み込み演算に必要なパラメータ数を削減し、モデルの計算効率を向上させる。`},
{target:`SENet Squeeze and Excitation Network`, content:`2017年に発表された画像分類のためのディープラーニングモデル。<br>Convolutional Neural Network （CNN）のアーキテクチャーに、Squeeze-and-Excitation （SE）ブロックを組み込むことで、より高い性能を実現。SEブロックは、CNNの畳み込み層の後に配置されるブロックで、チャンネルごとの重要性を学習することで、より効率的な特徴抽出を行う。SEブロックは、2つのステップ、squeezeとexcitationから構成される。squeezeは、チャンネルごとの特徴マップに対して、Global Average Poolingを行い、1次元のベクトルに変換する。excitationは、squeezeで得られたベクトルを入力として、2つの全結合層を通し、その出力をsigmoid関数に通した値を用いて、各チャンネルごとの重要性を再調整する。これにより、より重要な特徴がより強調され、より精度の高い画像分類が実現できる。`},
{target:`SEブロック`, content:`畳み込み層の出力に対して、チャンネルごとに異なる重みを掛けることで、重要な特徴マップにより多くの重みを与えることができる。<br>具体的には、入力された特徴マップのグローバル平均を計算し、2つの全結合層で処理して、チャンネルごとに異なる重みを算出する。そして、その重みを特徴マップに掛け合わせることで、重要な特徴により大きな重みを与えることができる。このような機構を通じて、モデルはより効率的かつ正確に画像の特徴を抽出できる。このブロックは口と目の特徴量マップに強く活性化されている部分を検知したのに、目の特徴量マップの活性化の度合いが緩やかなら、目の特徴量マップをブーストする。正確には、無関係な特徴量マップを弱める。また、目が他のものと混同されていた場合、この特徴量マップの再較正を通じて曖昧さを取り除く。`},
{target:`バウンディングボックス bounding box`, content:`画像中の特定のオブジェクトや領域を囲む矩形の枠を指す。<br>物体の幅と高さ、中心の位置（x、y）の4個のメンバーを予測する。物体検出や顔検出、車両検出、人物追跡、姿勢推定などのタスクに使用。画像データ内の重要な領域を効率的に識別し、情報を抽出することが可能になる。`},
{target:`アノテーション annotation`, content:`データセットや文書に対して追加情報やラベルを付与すること。<br>画像にバウンディングボックスのこれをつけるには、VGG Image Annotator、LabelImg、OpenLabeler、ImgLabなどのオープンソースの画像ラベリングツールかLabelBoxかSuperviselyといった市販ツールを使うとよい。`},
{target:`IoU Intersection over Union`, content:`物体検出やセグメンテーションなどのコンピュータビジョンタスクで使用される評価指標。<br>予測された領域（バウンディングボックスやセグメンテーションマスク）と正解の領域との重なり具合を表す。tf.keras.metrics.MeanIoUクラスが実装している。`},
{target:`物体検知 object detection`, content:`画像やビデオ中の特定の物体の位置とクラスを同定するタスク。<br>画像内の複数の物体を同時に検出し、それぞれの物体に対してバウンディングボックス（境界ボックス）を推定する。`},
{target:`非極大抑制 Non Maximum Suppression NMS`, content:`物体検知やオブジェクト検出の後処理手法の一つ。<br>重なりのある複数のバウンディングボックスや検出領域の中から最も信頼性の高いものを選択するために使用。`},
{target:`全層畳み込みネットワーク Fully Convolutional Network FCN`, content:`画像処理のタスクにおいて広く使用される畳み込みニューラルネットワークの一種。<br>畳み込み層とプーリング層のみから構成され、全結合層（fully connected layer）がない。通常の畳み込みニューラルネットワークは、最終的な出力を得るために全結合層を使用するが、その代わりに畳み込み層を使って出力を生成する。これにより、入力画像の任意のサイズに対しても出力を生成することができる。`},
{target:`YOLO You Only Look Once`, content:`物体検出のためのリアルタイムなニューラルネットワークベースのアルゴリズム。<br>画像全体を一度の推論で処理し、同時に物体のバウンディングボックスとクラスの予測を行う。`},
{target:`セマンティックセグメンテーション semantic segmentation`, content:`画像処理のタスクの一つであり、画像内の各ピクセルに対してクラスラベルを割り当てる領域分割の手法。<br>つまり、画像をピクセルレベルでクラス分類することを目的としている。入力画像をピクセルごとに分類するため、各ピクセルに対してクラスを予測。予測結果は、通常はカラーマップなどの形式で表現され、各クラスに対応する色が割り当てられた出力マップ（セグメンテーションマップ）が得られる。同じクラスの異なる物体は区別しないことに注意。`},
{target:`インスタンスセグメンテーション instance segmentation`, content:`画像処理のタスクの一つであり、画像内の個々のオブジェクトをピクセルレベルで分割し、それぞれのオブジェクトに対して一意の識別子を割り当てる手法。<br>つまり、セマンティックセグメンテーションと同様に画像内の領域を分割するが、異なるオブジェクトが異なるインスタンスとして区別される。各オブジェクトの境界を正確に検出し、オブジェクトごとに異なるセグメンテーションマスク（マスク画像）を生成。これにより、画像内の異なるオブジェクトを個別に識別できる。この利点は、画像内の複数のオブジェクトを個別に識別することができること。例えば、画像中の複数の人物、車両、動物など、同じクラスに属するが異なるインスタンスを区別することが可能。自動運転、物体追跡、ロボティクス、人物検出と追跡、映画効果の追加など、さまざまなアプリケーションで使用されている。この手法には、セマンティックセグメンテーションと物体検出の手法を組み合わせたアプローチが一般的。一般的な手法には、Mask R-CNN、Panoptic FCN、YOLOv4+Panoptic、Detectronなどがある。これらの手法は、物体の境界を検出し、それぞれのオブジェクトに対して個別のセグメンテーションマスクを生成するためのモデルを提供する。`},
{target:`再帰型ニューラルネットワーク Recurrent Neural Network RNN`, content:`時系列データやシーケンシャルデータのモデリングに使用。<br>循環的な接続を持つネットワーク構造を持ち、前のステップの出力を現在のステップの入力として利用する。シーケンシャルデータの時間的な依存関係をモデル化する。時系列データのモデリングやシーケンシャルタスクの解決に有効なアーキテクチャ。長期的な依存関係や時間的なパターンを捉えることができ、音声認識、自然言語処理、機械翻訳などの多くのタスクで広く利用。`},
{target:`シーケンス処理`, content:`あらかじめ決められた順序で処理を行うこと。`},
{target:`自然言語処理 Natural Language Processing NLP`, content:`コンピュータが人間の自然言語を理解し、処理するための一連の技術と手法。<br>テキストデータや音声データなどの自然言語データを解析し、意味や情報を抽出したり、文書の分類や要約、翻訳、感情分析などのタスクを実行したりするために使用。言語の理解と生成の2つの主要な側面をカバーしている。言語の理解では、文や文書の意味や情報を抽出し、テキストの解釈や分類、関連性の判定、感情分析などを行う。一方、言語の生成では、与えられた情報に基づいて文や文書を生成したり、要約や翻訳などを行う。`},
{target:`時系列データ time series data`, content:`時間の経過に伴って収集されたデータの系列。<br>このデータは、時間軸上で観測される値やイベントの系列を表現する。例えば、株価の変動、気温の変化、センサーデータの計測値、ウェブトラフィックの推移など。このデータは、時間的な依存関係やパターン、トレンド、周期性を持つことが一般的。これにより、過去のデータから将来の予測や分析を行うことが可能。`},
{target:`タイムステップ time step`, content:`時系列データの中での1つの単位時間のこと。<br>時系列データは時間の経過に応じて観測されるデータの系列であり、各データポイントは特定のこのステップに対応。`},
{target:`再帰ニューロン`, content:`ニューラルネットワークにおいて、再帰的な接続を持つニューロンのこと。`},
{target:`記憶セル`, content:`再帰型ニューラルネットワーク（RNN）やその派生モデル（例: LSTM、GRU）において使用される構成要素の一つ。<br>時系列データの長期的な依存関係を保持し、過去の情報を記憶する。`},
{target:`シーケンス ツー シーケンス ネットワーク sequence to sequence network`, content:`入力シーケンスを別の出力シーケンスに変換するためのニューラルネットワークモデル。<br>このモデルは、時系列データや自然言語処理のタスクなど、シーケンスデータの変換や生成に広く使用。エンコーダとデコーダの2つの主要なコンポーネントから構成。エンコーダは入力シーケンスを固定次元のベクトル表現（コンテキストベクトルまたは隠れ状態）にエンコードし、デコーダは、エンコーダの出力と過去のデコーダの出力（開始トークンや前の予測）を使用して、ターゲットシーケンスを生成する。一般的な例は、機械翻訳。`},
{target:`シーケンス ツー ベクトル ネットワーク sequence to vector network`, content:`入力のシーケンスデータを固定次元のベクトル表現に変換する。<br>シーケンスデータから重要な情報を抽出し、固定次元の表現に圧縮するタスクに使用。一般的にエンコーダのみから構成。エンコーダは、入力されたシーケンスデータを逐次的に処理し、最終的にシーケンス全体を要約するベクトル表現を生成。このベクトル表現は、シーケンスデータの重要な特徴や意味を捉えることが期待。例えば、文書分類では、文書のテキストを入力とし、その文書のカテゴリを予測するためのベクトル表現を生成。また、感情分析や情報抽出など、自然言語処理の他のタスクにも適用。`},
{target:`ベクトル ツー シーケンス ネットワーク vector to sequence network`, content:`固定次元のベクトル表現を入力として受け取り、出力として可変長のシーケンスデータを生成する。<br>このモデルは、ベクトルからシーケンスへの変換や、文生成などのタスクに使用。一般的にデコーダのみから構成。デコーダは、入力されたベクトル表現を基に、逐次的にシーケンスデータを生成する。デコーダは、前の出力やコンテキスト情報を参照しながら、次の要素を予測してシーケンスを構築する。これにより、ベクトルから文や音声などのシーケンスデータを生成することができる。自然言語生成、音声合成、画像キャプション生成などのタスクに応用される。`},
{target:`エンコーダーデコーダー encoder decoder`, content:`主にシーケンスデータの変換や生成に使用されるニューラルネットワークアーキテクチャ。<br>入力データをエンコードして中間表現を得た後、その中間表現をデコードして目的の出力を生成する。`},
{target:`エンコーダー encoder`, content:`入力データ（例 画像、テキスト、音声）を受け取り、そのデータを抽象的な表現に変換。<br>データの特徴を抽出し、情報を圧縮して表現する。`},
{target:`デコーダー decoder`, content:`エンコーダーが生成した抽象的な表現を入力として受け取り、目的の出力を生成する。<br>エンコーダーの中間表現を逆に展開し、情報を解読して元のデータや目的の出力を生成する役割を担う。特にシーケンス生成や変換の場合に頻繁に利用。`},
{target:`BPTT Back Propagation Through Time`, content:`再帰型ニューラルネットワーク（RNN）やその派生モデルであるLSTM（Long Short-Term Memory）やGRU（Gated Recurrent Unit）において、時系列データを逆方向に伝播させるための学習アルゴリズム。<br>RNNの各時刻の計算結果を逆方向にたどりながら、誤差を過去の時刻に伝播させる。具体的には、まず最終時刻の出力と目標値との誤差を計算し、その誤差を使って最終時刻の重みを更新する。そして、最終時刻から逆向きに各時刻の誤差を計算し、それを用いて重みを更新する。`},
{target:`双曲線正接活性化関数 hyperbolic tangent activation function`, content:`ニューラルネットワークにおいて広く使用される活性化関数の一つ。<br>入力値を取り、それを非線形な形状で変換して出力。入力値が大きく正の値に近づくと出力は+1に近づき、入力値が大きく負の値に近づくと出力は-1に近づく。入力値が0に近い場合、出力は0に近づく。一般的にシグモイド活性化関数と比較して出力の範囲が広いため、情報の表現範囲が広がるという特徴がある。また、シグモイド関数と同様に非線形性を持っており、ニューラルネットワークの表現力を高める役割を果たす。`},
{target:`TimeDistributed層`, content:`時系列データや系列データのバッチ全体に対して同じ操作を適用するためのKerasのラッパー層。<br>時系列データを処理するためのニューラルネットワークの構成要素。この層は、入力シーケンスの各時間ステップに対して同じ重みを共有することで、シーケンス全体に対する一貫した処理を実現する。時系列データの各時間ステップに同じ処理を適用する場合に有用。`},
{target:`層正規化 layer normalization`, content:`ニューラルネットワークの学習において使用される正規化手法の一つ。<br>通常のバッチ正規化（Batch Normalization）とは異なり、ミニバッチではなく、各層の内部で正規化を行う。`},
{target:`LSTM Long Short Term Memory`, content:`再帰型ニューラルネットワーク（RNN）の一種。主に時系列データやシーケンスデータの処理に使用。<br>RNNの学習中に起こりやすい勾配消失問題を解決するために設計。時系列データ内の長期的な依存関係を捉えることができる。通常のRNNでは、過去の情報が時間の経過とともに急速に失われてしまうという問題があるが、長期的なメモリを保持し、必要に応じて情報を更新することができる。ゲート（Gate）と呼ばれる仕組みを使用して情報の流れを制御。主なゲートには、忘却ゲート（Forget Gate）、入力ゲート（Input Gate）、出力ゲート（Output Gate）があり、各ゲートは、入力データや前の状態に基づいて、情報の流れや更新の度合いを調整する。具体的には、セル内部にはセル状態（Cell State）と呼ばれるメモリがあり、ゲートの制御によって情報の追加、削除、更新をする。セル状態は、長期的な依存関係を保持し、情報の流れを制御する役割を果たす。また、各時間ステップでセル状態から出力を計算し、次の時間ステップに渡すこともできる。自然言語処理（NLP）、音声認識、時系列予測などのタスクで広く使用される。`},
{target:`GRU Gated Recurrent Unit`, content:`再帰型ニューラルネットワーク（RNN）の一種。主に時系列データやシーケンスデータの処理に使用される。<br>LSTMと同様に長期的な依存関係を捉えるために設計されているが、LSTMよりもパラメータ数が少なく計算効率が高いという特徴がある。ゲート（Gate）と呼ばれるメカニズムを使用して情報の流れを制御する。LSTMと同様に、忘却ゲート（Forget Gate）と入力ゲート（Input Gate）があるが、セル状態（Cell State）が存在せず、隠れ状態（Hidden State）のみが出力として利用される。`},
{target:`WaveNet`, content:`「膨張率」（dilation rate）を倍にしながら「1次元畳み込み層」を積み上げる。<br>最初の「畳み込み層」は1度に2タイムステップ、次の「畳み込み層」は4タイムステップ、次の「畳み込み層」は8タイムステップをみる。「下位層」は短いパターン、「上位層」は長いパターンを学習する。「膨張率」を次々に倍にしているため、このネットワークはきわめて長い「シーケンス」を効率よく処理できる。実際に「膨張率」が1,2,4,8,...,256,512の10個の「畳み込み層」を積み上げ、同様の10個の「畳み込み層」による「ブロック」（「膨張率」は同じく1,2,4,8,...,256,512）をさらに2つ積み上げている。ネットワーク全体で「シーケンス」の長さが同じになるように、各層の「入力シーケンス」の先頭に「膨張率」に合わせて適切な数のゼロを「パディング」している。音声合成や音声処理のためのディープラーニングモデル。Google DeepMindによって開発され、2016年に発表。高品質かつ自然な音声を生成することができることで注目を集めた。生成モデルとして知られる条件付き確率モデルの一種。音声波形を直接モデル化し、音声のスペクトログラムなどの中間表現を使用せずに音声の生成を行う。モデルは、畳み込みニューラルネットワーク（CNN）と双方向リカレントニューラルネットワーク（RNN）の組み合わせに基づいている。`},
{target:`膨張率`, content:`個々の「ニューロン」の入力が離れているかを示す。`},
{target:`causalパディング`, content:`入力の先頭に適切な数のゼロをパディングしつつ、validパディングするのと同じ。`},
{target:`文字RNN`, content:`文字レベルのリカレントニューラルネットワーク（Recurrent Neural Network）。`},
{target:`ステートレスRNN`, content:`リカレントニューラルネットワーク（RNN）の一種。過去のステータス（状態）を保持しないRNNのこと。<br>各入力の処理が個別に行われ、過去のステータスを保持しないため、入力ごとに独立していると考えることができる。入力の各要素を個別に処理するため、並列計算が可能であり、高速な処理が期待できる。また、長期的な依存関係を持つデータでは、情報の欠落や勾配消失の問題が起こりにくい。ただし、過去の情報を考慮しないため、一部のタスクでは性能が低下する。`},
{target:`定常的`, content:`そのデータが統計的に安定している性質を指す。<br>定常性がある時系列データは、時間の経過に伴って統計的な性質が変化しないという特徴。`},
{target:`T BPTT Truncated Back Propagation Through Time`, content:`時系列データにおける誤差逆伝播法（Backpropagation Through Time, BPTT）の一種。時系列データを学習するための手法。<br>ネットワークを展開して時間方向に逆向きに伝播させることで、時間的な依存関係を学習する。一定の時間範囲（トランク）に切り取って処理を行う。これにより、長い時系列データを扱う場合でも計算やメモリの制約を緩和し、効率的な学習が可能。適切なトランクサイズを選択することで、計算効率と学習性能のトレードオフを調整することがでる。ただし、トランクサイズが小さすぎると長期的な依存関係を捉えられない可能性があり、逆に大きすぎると計算やメモリの負荷が増える可能性がある。`},
{target:`ネスト`, content:`データ構造において、ある構造の内部に同じ構造が含まれている状態。`},
{target:`平坦化 Flattening`, content:`データ構造を一次元のフラットな形式に変換。`},
{target:`独立同分布 Independentand Identically Distributed IID`, content:`統計学と確率論において使用される概念。<br>複数の確率変数が互いに独立であり、かつ同じ確率分布に従っていることを指す。`},
{target:`温度 temperature`, content:`テキスト生成タスクにおいて確率分布から次の文字をサンプリングする際に使用されるパラメータ。<br>通常、モデルの出力として得られる確率分布は、各文字の出現確率を表す。温度を適用することで、確率分布のサンプリングの多様性を制御できる。具体的には、温度が高い場合（例えば、2以上の値）、確率分布がより平坦化され、より多様な文字がサンプリングされる。一方、温度が低い場合（例えば、0.5以下の値）、確率分布がよりピーク化され、より確信度の高い文字がサンプリングされる傾向がある。温度は、tf.math.log（y_proba） / temperatureのように確率分布の対数を温度で割ることで適用すｒｙ。この操作により、確率分布の各要素のスケールが変化し、サンプリングされる文字のバリエーションが調整される。温度の値の選び方はタスクや望ましい出力の多様性によって異なり、高い温度はよりランダムな出力をもたらし、低い温度はより確信度の高い出力をもたらす傾向がある。適切な温度を選ぶことで、望ましい出力の特性や多様性を制御できる。`},
{target:`ステートフルRNN`, content:`リカレントニューラルネットワーク（RNN）の一種。過去のステータス（状態）を保持し、次のステップの処理においてその情報を利用するRNN。<br>1つの「訓練パッチ」の処理が終わっても、その「最終状態」を残し、つぎの「訓練パッチ」の「初期状態」として使う。各ステップの処理で前のステップの出力の代わりに、前のステップの隠れ状態（hidden state）やセル状態（cell state）などのステータスを利用。過去のステータスを保持することで長期的な依存関係をモデリングする能力がある。長いシーケンスや文脈を扱う場合に優れた性能を発揮。言語モデリング、音声認識、機械翻訳などのタスクで広く使用される。代表的なのの一つには、LSTM（Long Short-Term Memory）がある。`},
{target:`感情分析 sentiment analysis`, content:`自然言語処理の一分野でテキストデータから感情や意見を抽出するための技術。<br>テキストデータがポジティブ、ネガティブ、またはニュートラルな感情を持っているかを識別する。`},
{target:`デコード`, content:`一定の規則や方式に基づいて符号（コード）の集まりに変換されたデータに対し、符号化時とは逆方向の変換を行い、元のデータを復元すること。`},
{target:`バイトペアエンコーディング Byte Pair Encoding BPE`, content:`テキストデータをサブワード（部分単語）に分割するための手法。<br>低頻度の文字や単語に対しても効果的なトークン化を実現するため、自然言語処理のタスクで広く使用されている。手法としては、①テキストデータの初期状態では、各文字（または単語）が個別のトークンとして扱う。②テキストデータ内の連続する文字の組み合わせを頻度に基づいてマージする。最初に、テキストデータ内の文字の出現頻度を数える。③出現頻度が最も高い文字のペア（バイトペア）を見つけ、それを新しいサブワードとして結合。このとき、新しいサブワードは一意なトークンとして扱う。④テキストデータ内の全てのバイトペアを新しいサブワードに結合した後、新しいサブワードの出現頻度を再計算する。⑤出現頻度が最も高いサブワードのペアを新しいサブワードとして結合し、頻度の再計算を行う。このプロセスは、指定された回数のマージ操作（ハイパーパラメータとして設定）が完了するまで繰り返す。トークン化は、テキスト内の単語やフレーズの部分的な出現をキャプチャするため、未知の単語や固有名詞に対しても柔軟な対応が可能。また、異なる言語間での単語分割の一貫性や、語彙の効率的な利用にも役立つ。`},
{target:`可変長テンソル variable length tensor`, content:`要素の数や次元の長さが一定ではないテンソル。`},
{target:`密テンソル dense tensor`, content:`すべての次元のサイズが定義され、すべての要素が存在するテンソル。`},
{target:`語彙外 Out of Vocabulary OOV`, content:`自然言語処理やテキスト処理の文脈で使用される用語。<br>訓練データや辞書などの言語モデルの語彙に含まれていない単語やトークン。`},
{target:`マスキング`, content:`特定のデータ要素を無効化することで、モデルがそれらの要素を無視するようにするために使用。<br>これにより、モデルが特定の情報を考慮せずに予測や処理を行うことができる。`},
{target:`埋め込み層 embedding layer`, content:`自然言語処理やテキストデータの処理において使用される一種の層。<br>この層は、テキストデータ中の単語やトークンを密なベクトル表現（埋め込みベクトル）に変換。`},
{target:`パディングトークン`, content:`データ処理や機械学習の文脈で使用される用語。<br>データの一部を埋めるために使用される特殊なトークン。`},
{target:`マスクテンソル`, content:`データの一部を隠すために使用されるテンソル（行列）で、通常、0と1の要素から構成されるバイナリ行列。<br>このテンソルの1の要素は、対応するデータ要素をマスク（隠す）ことを意味し、0の要素はマスクされないことを示す。`},
{target:`事前訓練済みモデル`, content:`大規模なデータセットで事前に学習された機械学習モデル。`},
{target:`モジュール`, content:`ソフトウェアやプログラムの構成要素であり、特定の機能やタスクを実現するために独立して作成された部分。<br>関連する機能をまとめ、独自のインターフェースを提供。再利用できるコンポーネント。`},
{target:`文エンコーダ`, content:`自然言語処理のタスクにおいて、テキストデータ（文章）を数値表現に変換する役割を持つコンポーネントやモデルのことを指す。<br>文章の意味や情報を保持しつつ、機械学習モデルに入力するための数値表現（ベクトル）を生成する。`},
{target:`コーパス`, content:`自然言語処理や言語研究のために収集された大規模なテキストデータの集合体。<br>それぞれの単語をユニークな数値でエンコードする。似たような単語が似たようにエンコードされる、効率的で密な表現が得られる。実際の言語の使用例を反映しており、単語や文の出現パターンや文脈を分析するためのデータソースとして利用できる。`},
{target:`単語埋め込み`, content:`自然言語処理のタスクにおいて、単語を意味的なベクトル表現に変換する手法やモデルを指す。<br>単語の意味や関係性を数値表現として捉えるために使用できる。従来の単語表現方法である「one-hotベクトル」や「単語の出現頻度」などとは異なり、密なベクトル表現を提供する。このような密なベクトル表現は、単語間の意味的な関係を反映しており、機械学習モデルにおいてより有用な特徴として利用できる。`},
{target:`文埋め込み`, content:`自然言語処理のタスクにおいて、文全体を意味的なベクトル表現に変換する手法やモデルを指す。<br>文章の意味や文脈を数値表現として捉えるために使用される。単語埋め込み（Word Embeddings）の拡張として考えることができ、単語埋め込みでは、各単語を密なベクトル表現に変換するが、単語レベルの情報を越えて文全体をベクトル表現にエンコードする。`},
{target:`文解析モデル`, content:`自然言語処理（NLP）のタスクにおいて、テキストデータの解析や処理を行うために設計されたモデルやアルゴリズム。<br>テキストの構文、意味、文脈などを理解し、情報抽出、機械翻訳、感情分析、質問応答などのタスクを実行するために使用。`},
{target:`ニューラル機械翻訳モデル neural machine translation model`, content:`機械翻訳のタスクをニューラルネットワークを用いて実現するモデル。`},
{target:`ワンステップシフトバック one step shift back`, content:`系列データや時系列データの予測モデルにおいて、未来の予測値を現在の時点に戻す操作を指す。<br>通常、予測モデルでは現在の時点から未来の時点への予測が行われるが、1ステップシフトバックでは予測値を1ステップ分だけ前にずらし、予測値を過去の時点に対応させる。この手法は、予測値を現在の時点からのみ得られる情報だけでなく、未来の情報も考慮するために使用される。たとえば、系列データの次の値を予測する場合、現在の値のみでなく、将来の値が既知の場合に予測が改善される可能性がある。1ステップシフトバックを適用することで、モデルは未来の情報を現在の時点に反映させることができる。`},
{target:`出力語彙 output vocabulary`, content:`自然言語処理のタスクにおいて、モデルが生成する可能性のある単語の集合を指す。<br>モデルは、入力文や文脈から予測された確率分布を基に、この中から最適な単語を選択して生成する。`},
{target:`サンプリングソフトマックス softmax sampling`, content:`確率的に単語やアイテムをサンプリングする手法の一つ。確率的なサンプリングを用いてモデルの出力を生成する手法の総称。<br>自然言語処理や生成モデルなどの領域で使用。通常のソフトマックス損失関数では、全てのクラスの確率を計算する必要があるが、クラスの数が非常に大きい場合には計算コストが高くなるため、計算効率を改善する必要がある。tf.nn.sampled_softmax_loss（）は、この問題を解決するために、ランダムにサンプリングされたクラスのサブセットに対してのみソフトマックス損失を計算する。`},
{target:`ロジット`, content:`確率 pに対して、その事象が起こる確率を起こらない確率で割った値をオッズ（odds）と呼び、そのオッズの対数のこと。<br>確率pのオッズ（p/（1 × p））の対数。`},
{target:`サンプラ`, content:`データセットからサンプルを抽出するための機構やクラスのこと。`},
{target:`双方向再帰層 bidirectional recurrent layer`, content:`自然言語処理や音声処理などのタスクで広く使用されるリカレントニューラルネットワーク（RNN）の一種。<br>通常のRNNは、入力データを順方向に処理するだけだが、この層では、入力データを順方向と逆方向の両方で処理する。時系列データの文脈を考慮するために有用。通常のRNNでは、過去の情報に基づいて現在の予測を行うが、この層では、過去の情報だけでなく未来の情報も考慮に入れることができる。これにより、より豊かな文脈を捉えることができ、モデルの性能向上につながる。`},
{target:`ビームサーチ beam search`, content:`機械翻訳や自然言語処理のタスクにおいて、確率的なシーケンス生成を行う際に使用されるアルゴリズム。<br>候補となるシーケンスを効率的に探索し、最適なシーケンスを選択する手法。探索空間を制限することで効率的な探索を実現。具体的には、各ステップで最も確率の高い候補のみを保持し、その候補からさらに次のステップへの選択肢を生成する。`},
{target:`ビーム幅 beam width`, content:`ビームサーチ（Beam Search）アルゴリズムにおいて、各ステップで保持する候補の数を指定するパラメータ。<br>ビーム幅は、探索空間の制約を決定し、最終的な結果に影響を与える。`},
{target:`注意機構 attention mechanism`, content:`自然言語処理や画像処理などの機械学習タスクにおいて使用される一般的な手法。<br>入力のさまざまな要素に対して異なる重要度（注目度）を割り当てることにより、モデルがより重要な情報に焦点を当てることが可能。機械翻訳、文書分類、画像キャプション生成などのタスクで広く使用。`},
{target:`Attention層`, content:`注意機構（Attention Mechanism）をニューラルネットワーク内に組み込んだ層。<br>入力の重要な情報に対して重み付けを行い、それを利用して出力を計算する。`},
{target:`Bahdanau注意`, content:`機械翻訳や自然言語処理のタスクで使用される注意メカニズムの一つ。この注意メカニズムは、エンコーダ・デコーダモデルにおいて、デコーダの各ステップでエンコーダの出力に注目するための方法。<br>デコーダの各ステップでエンコーダの出力とデコーダの隠れ状態を使用して、関連性スコアを計算する。関連性スコアは、エンコーダの出力とデコーダの隠れ状態の類似度を測るために使用される。エンコーダとデコーダの間の長距離の依存関係や、異なる単語間の対応関係を捉えることができる。これにより、より正確で自然な翻訳や生成が可能。「エンコーダ」の出力と「デコーダ」の前の「隠れ状態」を連結しているので「連結注意」（additive attention）とか「加法注意」（additive attention）と呼ぶこともある。`},
{target:`一般ドット積アプローチ`, content:`注意メカニズムの一部として使用される、エンコーダの出力とデコーダの隠れ状態との関連性を計算する手法。<br>エンコーダの出力とデコーダの隠れ状態を要素ごとに掛け合わせて足し合わせた後、スカラー値の関連性スコアを計算する。`},
{target:`ドット積`, content:`ベクトル演算の内積のこと。`},
{target:`ビジュアル注意 visual attention`, content:`画像やビデオなどの視覚情報を処理する際に使用される注意メカニズムの一つ。通常、画像キャプション生成や画像の質問応答などのタスクにおいて利用される。<br>画像の特徴マップやフレームのセットなどのビジュアル情報と、テキストの文脈（例えば、RNNによる自然言語モデル）の間の関連性を計算するために使用。この関連性を計算することにより、モデルは画像内の特定の領域に注目することができる。`},
{target:`説明可能性 explainability`, content:`機械学習や人工知能のモデルやシステムが自身の意思決定や予測結果を理解可能で透明に説明する能力。<br>モデルやシステムの内部のメカニズムや要因を理解し、ユーザーやステークホルダーに結果を説明することで、モデルの信頼性や適用範囲を高めることができる。`},
{target:`Transformer`, content:`2017年に発表された革新的なニューラルネットワークアーキテクチャであり、自然言語処理や他の系列データ処理のタスクで非常に成功を収めている。<br>従来のリカレントニューラルネットワーク（RNN）ベースのモデルに代わる新しいモデルであり、注意機構（Attention Mechanism）を中心に構築。エンコーダとデコーダの2つの主要な部分から構成。エンコーダは入力シーケンスをエンコードし、デコーダはエンコードされた情報を使用して出力シーケンスを生成。機械翻訳、文章生成、質問応答、感情分析など、多くの自然言語処理タスクで優れた成果を上げている。`},
{target:`多頭注意 multi head attention`, content:`Transformerモデルなどのニューラルネットワークアーキテクチャで使用される注意メカニズムの一種。<br>個々の単語と同じ文のほかのすべての単語との関係を「エンコード」し、もっとも関係の深い単語に「注意」を払う。入力の異なる表現を複数の異なる「頭」（ヘッド）に渡すことで、より豊かな表現力と柔軟性を持つ注意機構を実現する。通常の注意機構では、単一の注意メカニズムがエンコーダとデコーダの間の関連性を計算するが、これは複数の異なる注意メカニズム（頭）が同時に計算される。各頭は、異なる線形変換（重み行列）を使用して、入力の異なる表現を作成する。`},
{target:`マスク済み多頭注意層 masked multihead attention layer`, content:`Transformerモデルなどのニューラルネットワークアーキテクチャにおいて、自己注意メカニズムを使用する際に適用される注意層の一種。<br>通常の多頭注意層では、各要素の関連性を計算する際に、全ての要素のペアに対して関連性スコアが計算される。しかし、シーケンスデータの場合、未来の情報を利用することはできない。そのため、学習の際に未来の情報を利用しないようにするためのマスクが必要。マスキング（マスク処理）を行うことで未来の情報を無視するようにする。`},
{target:`自己注意 self attention`, content:`自然言語処理タスクや画像処理タスクなど、さまざまな機械学習タスクで使用される注意メカニズムの一種。<br>シーケンスや画像の異なる要素間の関連性を計算し、各要素が他の要素にどれだけ関連しているかを表現する。`},
{target:`縮小ドット積注意 scaled dot product attention`, content:`Transformerモデルなどのニューラルネットワークにおいて使用される注意メカニズムの一つ。<br>入力クエリ（Query）とキー（Key）の類似度を計算し、その類似度に基づいて値（Value）の重み付き和を求める。具体的には、入力クエリとキーのドット積を計算し、スケーリングファクター（スケール）によって除算することで類似度を計算する。スケーリングは、ドット積の値の大きさを制御し、注意の安定性や効果を向上させるために導入される。具体的には、「クエリ」と「辞書内」の個々の「キー」との間で「類似度」を計算し、「ソフトマックス関数」でこれらの「類似度スコア」を「重み」（すべての「重み」を足すと1になる）に変換する。この処理全体は微分可能な「辞書ルックアップ」と考えることができる。`},
{target:`位置埋め込み positional embedding`, content:`Transformerモデルなどのシーケンス処理モデルにおいて、単語やトークンの位置情報を表現するために使用される技術。<br>単語の文のなかでの位置を「エンコード」する密ベクトルである。i番目の「位置埋め込み」が、文のi番目の単語の「単語埋め込み」に単純に追加される。Transformerモデルでは、自己注意（Self-Attention）メカニズムを使用してシーケンスデータを処理するが、自己注意は各要素の関連性を計算する際に位置情報を考慮しないため、位置情報を明示的にモデルに提供する必要がある。これを実現するために使用される。シーケンス内の各要素（単語やトークン）の位置に応じて学習可能なベクトルを割り当てる。一般的には、正弦関数や余弦関数などの周期的な関数を使用して、位置情報をエンコードする。この埋め込みベクトルは、モデルの入力として使用される単語埋め込み（Word Embedding）と結合され、位置情報が組み込まれた最終的な特徴表現を形成する。シーケンス内の要素の順序や位置に関する情報をモデルに提供するため、Transformerモデルが長いコンテキストを効果的に処理し、シーケンスタスクにおいて高い性能を発揮するのに役立つ。モデルがシーケンス内の要素の相対的な位置関係を把握するのに重要な役割を果たす。`},
{target:`LLM Large Language Models`, content:`自然言語処理（NLP）の分野で使用される、巨大な規模の機械学習モデルの一種。<br>大量のテキストデータを学習し、自然言語の理解、生成、翻訳、要約などのタスクを実行するために設計。特徴としては、①超大規模なモデル：数百万から数百億のパラメータを持つ非常に大きなニューラルネットワークで構成、膨大な量のテキストデータを学習し、高度な自然言語処理タスクに対応可能、②転移学習：大量のテキストデータを用いて事前訓練され、一般的な言語知識を獲得しており、比較的少ないラベル付きデータでファインチューニングが可能、③複数の言語対応：複数の言語に対応でき、翻訳、多言語検索、言語理解などのタスクに利用可能、④文脈理解：文脈を理解し、テキストの前後の文脈を考慮に入れて適切な応答や生成を行う能力を持ち、自然な対話や文章生成が可能、⑤問題解決：質問応答、要約、文章生成、言語モデリングなど、さまざまな自然言語処理タスクに適用でき、情報検索や文章生成、文書分類などのアプリケーションで利用価値が高る。一般的な例は、GPT-3、BERTなど。`},
{target:`ELMo Embeddings from Language Models`, content:`コンテキスト依存の単語埋め込み（Word Embedding）を生成するための手法。<br>事前訓練された言語モデルを使用して、単語の表現を学習し、文脈に依存した埋め込みを生成する。従来の単語埋め込み手法では、単語ごとに一意のベクトル表現が割り当てられるが、文脈によって単語の意味や表現が変化することを考慮している。双方向の再帰ニューラルネットワーク（Bidirectional LSTM）や畳み込みニューラルネットワーク（CNN）などの言語モデルを使用して、文脈情報をモデル化する。`},
{target:`ULMFiT Universal Language Model Fine Tuning`, content:`自然言語処理（NLP）タスクにおいて高い性能を発揮するための手法の一つ。<br>大規模な一般的なテキストコーパスで事前訓練された言語モデルを利用し、タスク固有のデータでファインチューニングを行う。この特徴は、一般的なテキストデータでの事前訓練とタスク固有のファインチューニングの組み合わせにある。これにより、少量のタスク固有のデータセットでも良好な性能を達成できる。また、一般的な言語モデルの事前訓練により、モデルは一般的な言語理解の能力を獲得し、特定のタスクにおいてもより強力な表現を学習することができる。`},
{target:`GPT Generative Pre trained Transformer`, content:`OpenAIの研究者たちが開発。「Transformer」風の「アーキテクチャ」を使っている。<br>「自己学習」により、大規模な「データセット」で12個の「Transformerモジュール」（ただし、「マスク済み多頭注意層」だけを使っている）の「スタック」という大規模だがかなり単純な「アーキテクチャ」を「事前学習」させた。さまざまな「NLPタスク」のための微調整を加えたが、それは個々の「タスク」に合わせて小規模な修正を加えている。具体的には、①言語モデルの事前訓練 大規模なテキストコーパスを使用して、Transformerモデルを事前訓練する。Transformerは、自己注意メカニズムを活用して文脈を捉える能力に優れている。この事前訓練の過程では、次の単語を予測するタスクや欠損した単語を予測するタスクなど、教師なしのタスクが使用される。②ファインチューニング 事前訓練されたモデルを、タスク固有のデータセットでファインチューニングする。具体的なタスクに合わせて、追加のレイヤーや重みを追加し、モデルを特定のタスクに適応させる。`},
{target:`GPT-2 Generative Pre trained Transformer2`, content:`OpenAIによって開発された事前訓練された言語モデル。<br>GPTの改良版であり、自然言語処理（NLP）タスクにおいて非常に高い性能を発揮する。大量のテキストデータを使用して教師なし学習による事前訓練を行っている。訓練の際には、Transformerアーキテクチャと自己注意メカニズムが活用され、文脈の理解と単語の予測能力を獲得している。特徴は、①モデルのサイズとパラメータ数は非常に大規模なモデルであり、パラメータ数が数億に及ぶ。これにより、より複雑な文脈の理解と言語生成が可能。②文章の生成と応答。与えられた文脈から文章を生成する能力を持っている。また、会話のコンテキストに基づいて応答を生成できる。そのため、自然な文章生成や対話型のタスクにおいて優れた性能を発揮する。③転移学習とファインチューニング。一般的な言語理解の能力を持っているため、さまざまなNLPタスクにおいて転移学習やファインチューニングが行える。事前訓練されたモデルを特定のタスクに合わせて調整し、高い性能を実現できる。`},
{target:`GPT-3 Generative Pre trained Transformer3`, content:`OpenAIが開発した最新の事前訓練された言語モデル。<br>前身のモデルであるGPT-2よりもさらに大規模で強力なモデルとなっており、自然言語処理（NLP）タスクにおいて驚異的な性能を発揮する。大量のテキストデータを使用して教師なし学習による事前訓練が行われている。訓練の際には、Transformerアーキテクチャと自己注意メカニズムを使用し、文脈の理解と単語の予測能力を獲得する。特徴は、①巨大なモデルサイズとパラメータ数：非常に大規模なモデルであり、パラメータ数は数千万から数百億にも及ぶ。この大規模なモデルは、より複雑な文脈の理解と言語生成能力を持ち、非常に高度なタスクに対応できる。②多様なタスクへの適用：文章生成、文章の要約、質問応答、翻訳、対話システムなど、さまざまなNLPタスクにおいて優れた性能を発揮。さらに、特定のドメインやタスクに応じてファインチューニングが可能。③驚異的な文章生成能力：与えられた文脈から非常に自然な文章を生成する能力を持っている。多くの場合、生成された文章は人間のような表現力を持ち、高い品質を実現。その大規模なモデルサイズと高い性能により、自然言語処理の研究や応用のさまざまな領域で革新的な成果を上げている。ただし、モデルの大規模さと計算リソースの要求のため、実際の運用には高いコンピューティングパワーが必要となることに留意。<br><br>GPT-3モデルの構造<br>①Transformerアーキテクチャ: GPT-3は、Transformerアーキテクチャをベースにしている。Transformerは、Attention Mechanismを中心としたネットワーク構造であり、自然言語処理タスクにおいて非常に効果的な構造。Transformerは、エンコーダーとデコーダーという2つの主要な部分から構成されるが、GPT-3はエンコーダー部分のみを使用。<br>②自己注意メカニズム: Transformerアーキテクチャの主要な要素の1つ。自己注意メカニズムは、入力の各要素間の関連性を計算するために使用され、文脈の理解や単語の予測能力を向上させる。<br>③スタックされたTransformerブロック: GPT-3は、複数のTransformerブロックがスタックされた形で構成。各Transformerブロックには、複数のMulti-Head AttentionレイヤーとFeedforward Neural Networkレイヤーが含まれる。<br>④事前学習とファインチューニング: GPT-3は、大規模なテキストデータセットを用いた事前学習を通じて学習。その後、特定のタスクに適応するために、ファインチューニングを実施。ファインチューニングには、タスク固有のデータセットでモデルを微調整するプロセスが含まれる。`},
{target:`BERT Bidirectional Encoder Representations from Transformers`, content:`Googleが2018年に発表した事前訓練された言語モデル。<br>自然言語処理（NLP）のタスクにおいて革新的な成果を上げ、特にテキストの表現学習と理解において大きな進歩をもたらした。Transformerアーキテクチャを基盤としており、教師なしの大規模なコーパスを使用して事前訓練される。訓練の際には、文脈を理解するために双方向のモデルが使用され、文中の単語や文の関係を学習する。この事前訓練により、単語や文の意味的な表現を獲得し、多様なNLPタスクに適用することができる。特徴は、①双方向の表現学習：文脈の両方向の情報を考慮して単語の表現を学習する。これにより、単語の意味や文脈に基づいた表現を獲得。②転移学習とファインチューニング：一般的なNLPタスクにおいて事前訓練されたモデルを使用し、特定のタスクに適応させることができる。転移学習により、タスクごとのデータセットを使わずに効果的なモデルを構築できる。③文脈の理解と意味の表現：文脈の理解と単語の意味的な表現を獲得するため、様々なNLPタスクで高い性能を発揮する。文の関係や単語の類似度など、さまざまな言語理解タスクにおいて優れた結果を示す。文章分類、質問応答、文書要約、機械翻訳など、さまざまなNLPタスクにおいて広く使われている。また、これをベースにした改良版や応用モデルも開発されており、自然言語処理の研究や応用の分野で重要な役割を果たしている。`},
{target:`オートエンコーダ Autoencoder`, content:`ディープラーニングの一種であり、非監視学習アルゴリズムの一つ。入力データを効率的に圧縮し、その再構築を行うことで、データの特徴を学習することを目的としている。エンコーダとデコーダと呼ばれる2つの主要な構成要素から構成され、エンコーダは、入力データをより低次元の表現に変換する役割を果たす。デコーダは、エンコーダの出力を再構築し、元の入力データに近い形式に戻す役割を担う。学習は、再構築誤差を最小化するように行われ、エンコーダは入力データの重要な特徴を抽出する能力を獲得し、デコーダはその特徴を用いて元のデータを再構築することができるようになる。データの次元削減やノイズ除去、特徴抽出などのタスクに利用される。また、生成モデルの一部としても使用され、新しいデータの生成や異常検知などの応用がある。`},
{target:`特徴量検出器 feature detector`, content:`データセットや画像などの入力から特徴的なパターンや構造を検出するために使用されるアルゴリズムやモジュール。<br>機械学習やコンピュータビジョンの分野で広く利用されている。その目的は、入力データの中から重要な情報を抽出することで、データをより意味のある表現に変換する。これにより、データの分類、認識、異常検知などのタスクを実行する際に役立つ。様々な手法で実装されることがあり、例として、コンピュータビジョンの分野では、エッジ検出、角度検出、テクスチャ解析、スケール不変特徴変換（SIFT）や特徴マッチングなどが一般的な手法として用いられる。出力は、通常は特徴量ベクトルや特徴マップの形式で表される。これらの特徴量は、後続の処理やアルゴリズムに供されることで、データのパターン認識や分析、予測などに利用される。`},
{target:`生成モデル generative model`, content:`データの生成プロセスをモデル化し、新しいデータを生成するために使用される統計モデルまたは機械学習モデル。<br>与えられたデータセットから学習し、そのデータの統計的な分布をモデル化する。このモデルを使用すると、既存のデータと類似した新しいデータを生成できる。確率モデルや深層学習モデルなど、さまざまなアプローチで実装される。教師なし学習の一種であるモデルも含まれ、データセットにラベルやターゲットが与えられていない場合に特に有用。データの潜在的な構造や特徴を学習し、その構造を使用して新しいデータを生成する能力を持っている。さまざまな応用に使用され、画像生成や音声合成、文章生成などの創造的なタスクに使用されるほか、データ拡張や欠損データの補完などのデータ処理のためにも利用できる。また、このモデルは異常検知やデータ品質管理などの応用にも活用される。例としては、敵対的生成ネットワーク（GAN）があり、ディープラーニングの手法であり、このモデルとして知られている。GANは、生成器と識別器と呼ばれる2つのネットワークを競わせることで、高品質なデータ生成を行う。`},
{target:`再構築 reconstruction`, content:`データの元の状態や形式に近い状態に戻すプロセスまたは操作。`},
{target:`再構築ロス reconstruction loss`, content:`オートエンコーダや生成モデルの訓練中に使用される損失関数の一つ。再構築の品質を評価するために使用される指標。<br>元のデータと再構築データとの間の誤差や差異を測定し、その値を最小化することを目指す。`},
{target:`不完備オートエンコーダ incomplete autoencoder`, content:`入力データの一部が欠損している状態で学習を行うオートエンコーダの一種。欠損したデータを補完する能力を持つことが特徴。<br>通常のオートエンコーダでは、入力データをエンコーダを介して潜在表現に圧縮し、その後デコーダを介して元のデータを再構築する。しかし、これは入力データの一部が欠損しており、その欠損した箇所を補完することが求められる。欠損したデータを推定するために訓練データのパターンや特徴を学習する。エンコーダは欠損したデータを潜在表現に変換し、デコーダは潜在表現を補完したデータに再構築する。この過程によって、不完全な入力データを元のデータに近い形式に再構築することを目指す。欠損データの補完や欠損データの予測などのタスクに使用される。例えば、画像データの一部が欠損している場合、これを使用して欠損箇所を補完することができる。また、欠損データの予測は、異常検知や欠損データの復元などの応用にも役立つ。訓練は、通常のオートエンコーダと同様に行われるが、欠損したデータのみを対象として学習が進められる。訓練データには、元のデータと欠損箇所がマスキングされたデータが使用されることが一般的。訓練時には、再構築ロスや欠損箇所の予測精度などが評価指標として使用される。欠損データの補完や予測において有用な手法である。`},
{target:`スタックオートエンコーダ stacked autoencoder`, content:`複数のオートエンコーダを積み重ねて構成されるニューラルネットワークの一種。各層の出力が次の層の入力として使用され、階層的な特徴表現を学習することが特徴。<br>通常、エンコーダとデコーダの間に追加の隠れ層（ボトルネック層）を持つ。最初のオートエンコーダは入力データを低次元の潜在表現に圧縮し、次のオートエンコーダはその潜在表現をさらに圧縮。このようにして、徐々により高度な特徴表現が獲得する。層ごとにオートエンコーダをトレーニングし、前の層からの出力を次の層の入力として使用する。最初の層のオートエンコーダは、入力データを圧縮するためにトレーニングされる。その後の層では、以前の層からの出力を入力として受け取り、より高度な特徴表現を学習する。「深層オートエンコーダ」（deep autoencoder）とも呼ぶ。`},
{target:`二項交差エントロピー損失関数 binary cross entropy loss function`, content:`2つのクラス分類問題において使用される損失関数の一つ。<br>主にシグモイド関数を通じて確率を出力するニューラルネットワークの最終層に適用。`},
{target:`t-SNE t-distributed Stochastic Neighbor Embedding`, content:`高次元データを可視化するための非線形次元削減手法の一つ。`},
{target:`重みの均等化 tied weights`, content:`ニューラルネットワークにおいて、異なる層間で同じ重みを共有するテクニック。<br>これにより、ネットワーク内の異なる部分が同じ特徴やパラメータを共有することができる。`},
{target:`畳み込みオートエンコーダ convolutional autoencoder`, content:`畳み込みニューラルネットワーク（CNN）を用いたオートエンコーダの一種。畳み込み層と逆畳み込み層を組み合わせて、画像や時系列データなどの特徴を抽出し、再構築することができる。<br>アーキテクチャ ①エンコーダ部分・・・畳み込み層、プーリング層 ②デコーダ部分・・・逆畳み込み層（転置畳み込み層）、アップサンプリング層。エンコーダ部分で入力データの特徴を抽出し、デコーダ部分で再構築する。訓練時には、入力データと再構築データの間の誤差を最小化するようにネットワークを学習させる。これにより、モデルは入力データの特徴を効果的に抽出し、再構築することができるようになる。画像処理や畳み込みニューラルネットワークにおいて広く使用。特に画像の特徴抽出やノイズ除去、画像生成などのタスクに有用。`},
{target:`再帰型オートエンコーダ recurrent autoencoder`, content:`再帰型ニューラルネットワーク（RNN）を用いたオートエンコーダの一種。RNNの特徴である時系列データやシーケンスデータの処理に適した構造を持ち、入力データの特徴を抽出し再構築することができる。<br>このアーキテクチャ ①エンコーダ部分・・・RNN層 入力データを時系列的に処理し、隠れ状態を保持。一般的にはLSTMやGRUなどの再帰型層が使用。隠れ状態 RNN層の出力や隠れ層の状態を表すベクトル。エンコードされた入力データの情報を保持。②デコーダ部分・・・RNN層（逆方向）エンコーダ部分で生成された隠れ状態を逆向きに処理。これにより、入力データの再構築を行う。出力層 デコーダ部分のRNN層の出力を用いて、入力データの再構築を行う。エンコーダ部分で入力データの特徴を抽出し、デコーダ部分で再構築を行う。訓練時には、入力データと再構築データの間の誤差を最小化するようにネットワークを学習。これにより、モデルは入力データの特徴を効果的に抽出し、再構築することができるようになる。音声処理、自然言語処理、時系列データの特徴抽出などのタスクに有用。RNNの長期的な依存関係のモデリング能力を活かし、データの時系列的なパターンや構造を捉えることができる。`},
{target:`RepeatVector層`, content:`与えられた入力を指定された回数だけ複製する。<br>主な用途は、時系列データやシーケンスデータを処理する際に、特定のタイムステップでの情報を全体のシーケンスに拡張する。この層の動作は単純で、入力を指定された回数だけ複製して返す。これにより、後続の層が各タイムステップで同じ情報を利用できるようになる。具体的な使用例としては、再帰型ニューラルネットワーク（RNN）のエンコーダ部分で最後の隠れ状態をこの層によって複製し、デコーダ部分でそれを複数のタイムステップに渡って利用する場合がある。これにより、エンコーダの情報をデコーダに伝えることができる。`},
{target:`ノイズ除去スタックオートエンコーダ denoising stacked autoencoder`, content:`スタックオートエンコーダの一種。入力データにノイズを付加して学習することで、ノイズの除去やデータの特徴抽出を行う手法。<br>ノイズ除去能力に優れており、ノイズの除去やデータの圧縮・復元などのタスクに有用。`},
{target:`スパースオートエンコーダ sparse autoencoder`, content:`オートエンコーダの一種。入力データの表現にスパース性（希薄性）を導入することを目指す手法。<br>①活性化関数の制約：通常の活性化関数に加えて、スパース性を導入するための制約が加えられる。代表的なスパース性制約の一つは、活性化関数にシグモイド関数ではなく、ReLU関数や双曲線正接関数（tanh）などを使用。②スパース性の促進：活性化関数の出力を制約するために、スパース性を促進する手法が使用。典型的な手法は、スパース正則化と呼ばれる項を損失関数に追加。スパース正則化は、活性化関数の出力のうち、一部のユニットのみが非ゼロであることを促す役割を果たす。③スパース性の調整：スパース性の度合いを制御するために、スパース性のパラメータを調整する必要がある。このパラメータは、スパース正則化の重みやスパース性の目標値などと関連付けられる。データ表現の次元削減や特徴抽出などのタスクにおいて、より解釈可能な特徴の獲得やノイズの除去、異常検知などに有用。スパース性の導入により、より効率的な特徴表現を獲得することが期待。`},
{target:`ActivityRegularization層`, content:`ニューラルネットワークの層の出力の活性化値に対して正則化を適用するための層。<br>これにより、出力のスパース性や活性化値の制約を導入することができる。`},
{target:`変分オートエンコーダ Variational AutoEncoder VAE`, content:`生成モデルの一種であり、データの潜在表現を学習するためのニューラルネットワークモデル。<br>教師なし学習の一形態として、主に次元削減やデータ生成、異常検出などのタスクに使用される。通常のオートエンコーダと比べて潜在表現に確率的な要素を導入することが特徴。具体的には、エンコーダとデコーダの2つの主要な部分で構成される。エンコーダは、入力データを潜在空間上の確率分布のパラメータ（平均と分散）にマッピングする。一般的に、このマッピングは、ニューラルネットワークによって実現される。エンコーダは、入力データを潜在表現に圧縮し、潜在空間上の確率分布を学習する。デコーダは、潜在空間のサンプルを入力として受け取り、元のデータの再構築を試みる。デコーダもニューラルネットワークによって実現され、潜在表現から元のデータの分布を復元することを目指す。学習は、最尤推定や確率的勾配降下法を用いて行われる。損失関数としては、再構築誤差と潜在空間の確率分布との間の距離（通常はクロスエントロピーまたはKLダイバージェンス）を最小化するように設計される。特徴的な利点は、潜在表現が連続的であるため、潜在空間上での操作や補間が可能であること。また、新しいデータの生成にも使用できる。ランダムな潜在ベクトルをサンプリングし、デコーダを通じて生成されたデータを得ることができる。「確率論的オートエンコーダ」（probalilistic autoencoder）であり、「生成的オートエンコーダ」（generative autoencoder）である。`},
{target:`確率論的オートエンコーダ probabilistic autoencoder`, content:`生成モデルの一種であり、データの潜在表現を学習するためのニューラルネットワークモデル。<br>通常のオートエンコーダに確率的な要素を導入することで、より柔軟なモデルを実現。`},
{target:`生成的オートエンコーダ generative autoencoder`, content:`データの生成モデルとしての機能を持つオートエンコーダ。<br>通常のオートエンコーダと同様に、エンコーダとデコーダの2つの主要なコンポーネントから構成されるが、主な目的はデータの生成。エンコーダは、入力データを低次元の潜在表現にマッピング。この潜在表現は、元のデータをより効率的に表現するための圧縮された表現。デコーダは、潜在表現を入力として受け取り、元のデータの再構築を試みる。通常、デコーダはエンコーダの逆操作を行い、潜在表現を元のデータの次元に戻す。このデコーダを使用して新しいデータを生成することができる。具体的には、ランダムな潜在ベクトルを入力として与えることで、デコーダを通じて新しいデータのサンプルを生成する。生成モデルとしての柔軟性とデータの表現学習の能力を組み合わせている。生成的モデルとしての機能を持つため、新しいデータの生成やデータの変換、補完、異常検出など、さまざまな応用が可能。ディープラーニングの生成モデルの一部として広く利用されている。また、生成モデルの進化や画像生成、音声生成、文章生成などの分野で活発な研究が行われている。`},
{target:`RBM restricted boltzmann machine`, content:`深層学習の生成モデルの一種であり、確率的なニューラルネットワーク。<br>可視変数と隠れ変数の間の結合を学習することで、データの生成、特徴抽出、異常検出などのタスクに利用。2つの層で構成されている。①可視層（visible layer）、②隠れ層（hidden layer）。可視層は観測されるデータを表し、隠れ層は可視層の裏側でのみ活動し、データの潜在的な特徴を表現する。生成モデルとしての性質を持っており、学習された結合のパターンに基づいて新しいデータを生成することができる。また、隠れ層のユニットはデータの特徴を学習するため、特徴抽出にも利用される。画像処理、音声処理、自然言語処理などの分野で広く利用されている。また、深層学習の前駆としても位置づけられ、深層信念ネットワーク（DBN）やディープボルツマンマシン（DBM）などのモデルの基礎となっている。`},
{target:`近似ベイズ推論 approximate bayesian inference`, content:`確率モデルに基づく統計的推論の手法の一つ。ベイズ推論では、事前分布とデータに基づく尤度関数を組み合わせて事後分布を計算し、パラメータの事後分布を推定する。<br>しかし、多くの場合、解析的に事後分布を求めることは困難なため、この推論が用いられる。事後分布を厳密に求める代わりに、近似的な分布を利用して推論する。一般的な近似手法には、変分推論、モンテカルロ法（MCMC）、期待値伝搬法（EP）、近似推論法（AIS）など。これらの手法は、事後分布に近い近似分布を推定することで、パラメータの推定や予測、モデルの比較などを行う。`},
{target:`変分ベイズ推論 variational bayesian inference`, content:`近似ベイズ推論の一種であり、確率モデルの事後分布を近似するための手法。<br>事後分布を解析的に求めることが困難な場合に特に有用。真の事後分布を近似するために、近似分布（変分分布）を導入する。この近似分布は、真の事後分布とできるだけ近くなるようにパラメータ化される。具体的には、近似分布と真の事後分布の間の距離を最小化するように近似分布のパラメータを求める最適化問題として定式化される。`},
{target:`平均コーディング meancoding`, content:`カテゴリカル変数を数値化する手法の一つ。<br>通常、カテゴリカル変数はバイナリ特徴量やOne-Hotエンコーディングによって表現されるが、このコーディングでは各カテゴリのレベルにおける目的変数の平均値を使って数値化する。`},
{target:`ガウス分布 gaussian distribution`, content:`正規分布（normal distribution）のこと。`},
{target:`コーディング空間 coding space`, content:`オートエンコーダや生成モデルにおいて、入力データの圧縮や特徴表現を表現するための空間。<br>入力データの特徴を抽象化し、低次元の表現に変換する役割を果たす。この空間の次元数は、オートエンコーダの中間層のユニット数によって制御される。高次元のこの空間では、より多くの情報が保持される可能性があるが、同時に過学習のリスクも高まる。一方、低次元のこの空間では情報の損失が生じる可能性があるが、データの特徴を簡潔に表現することができる。多くの場合、連続的な実数値を持つベクトル空間として表現される。このベクトル空間内の各点は、オートエンコーダによって学習された特徴表現を表す。この空間内の点を操作することで、データの生成や変換が可能となる。例えば、生成モデルではランダムな点をこの空間からサンプリングし、デコーダを通じて新しいデータを生成することができる。`},
{target:`潜在ロス latent loss`, content:`変分オートエンコーダ（VAE）などの生成モデルにおいて使用される損失関数の一部。<br>モデルが学習するべき潜在空間（latent space）の特性や性質を制約する役割を持つ。変分オートエンコーダでは、入力データをエンコーダを通じて潜在空間にマッピングし、そこからサンプリングした潜在変数（latent variable）をデコーダに入力して生成データを生成する。モデルが生成されたデータを潜在空間で良好に分布させるために使用される。一般的に、「クラスタリング」と「連続性」の2つの要素から構成される。①クラスタリング（クラスター化）・・・潜在空間内のデータ点が互いに近いべきであることを促進する。これにより、潜在空間内でデータを分類するクラスターが形成され、異なる特徴を持つデータが異なる領域に配置されるようになる。一般的な手法は、潜在空間内のデータ点間の距離や類似性を測るために使用されるクラスタリングアルゴリズム（例 K-means）を適用。②連続性・・・潜在空間内のデータ点が滑らかに変化するように促進される。これにより、潜在空間内の近くに位置するデータ点は似た特徴を持ち、データの変化が滑らかに反映されるようになる。一般的な手法は、潜在空間内のデータ点間の距離や類似性を測るために使用される距離関数やカーネル関数を適用。`},
{target:`KL情報量 Kullback Leibler divergence`, content:`確率分布間の距離や差異を計量するために使用される指標。<br>2つの確率分布の間の相対的な情報の差を測ることができる。変分オートエンコーダ（VAE）において、この情報量は潜在変数の確率分布と事前分布の間の差異を表すために使用。VAEは、エンコーダを使用して入力データを潜在空間にマッピングし、潜在変数をサンプリングしてデコーダに入力して生成データを生成する。このとき、潜在変数の確率分布は事前分布（通常はガウス分布）に近づけることが望ましい。この情報量を潜在ロスとしてVAEに組み込むことで、モデルは事前分布と潜在変数の分布の間の差異を最小化するように学習。VAEでは、通常、再構築ロス（入力データの再構築誤差）この情報量（「潜在ロス」）の2つの項からなる総合的な損失関数を最小化することで学習。`},
{target:`セマンティック補間 semantic interpolation`, content:`潜在空間上で異なる潜在表現の中間的なベクトルを生成する手法。<br>潜在空間は、変分オートエンコーダ（VAE）などで学習されたモデルの潜在表現を表す空間。潜在空間の2つの点（ベクトル）を選び、それらの点を結ぶ直線上の中間点を計算。この中間点に対応する潜在表現をデコーダに入力することで、新しいデータ点や画像を生成することができる。潜在空間の特定の方向に沿って変化するデータの変化を視覚化するために使用されることがある。例えば異なる顔の表情を滑らかに変化させたり、数字の形状を変化させたりするなど、データの潜在的な意味や特徴を可視化するのに役立つ。`},
{target:`敵対的生成ネットワーク GAN Generative Adversarial Network`, content:`生成モデルの一種であり、生成器（Generator）と識別器（Discriminator）と呼ばれる2つのネットワークが相互に競い合うことで学習を進める手法。<br>GANの学習は、生成器がデータを生成し、識別器が生成されたデータと本物のデータを区別するように学習。①生成器はランダムなノイズベクトルを入力として受け取り、それを使ってデータを生成。生成されたデータは本物のデータに似せるように学習。②識別器は生成器から生成されたデータと本物のデータを受け取り、それぞれが生成されたものか本物かを判別。識別器は二値分類器として設計されており、生成されたデータを1（本物）と判断するように学習。③生成器と識別器は相互に競い合う。生成器は識別器を騙すように生成データを改善し、識別器は生成器によって生成されたデータをより正確に識別できるように学習。④この競争と学習のプロセスが続き、生成器が本物のデータに近いデータを生成し、識別器が生成されたデータと本物のデータを区別できないほど学習が進んだ時点で、GANの学習は収束。GANは非常に強力な生成モデルであり、画像生成、音声合成、文章生成など様々なタスクで活用。`},
{target:`生成器 generator`, content:`敵対的生成ネットワーク（GAN）内のネットワークの一部。<br>ランダムなノイズから入力を受け取り、それを用いてデータを生成する役割を持つ。データ生成のために学習。具体的には、入力されたノイズベクトルを元に、本物のデータと似たデータを生成。例えば、画像生成の場合、ランダムなノイズベクトルを受け取り、それをディープニューラルネットワークを通じて変換し、画像として出力。訓練データセットに含まれる本物のデータに近いデータを生成するように学習。`},
{target:`潜在表現 latentre presentation`, content:`データの抽象的な表現であり、データの重要な特徴や意味を捉えるために用いられる。<br>データの高次元の表現から低次元の表現に変換される。特徴抽出やデータ圧縮などのタスクで使用される。例えば、画像データでは、画像の内容やスタイルを表現する特徴を抽出するのに役立つ。自然言語処理の場合、テキストでは文章の意味や感情を表現する特徴を捉えるのに使用される。通常は低次元の連続的なベクトル空間で表現される。このようなベクトル空間では、類似したデータは近くに配置され、異なるデータは遠くに配置される。データの持つ重要な情報を抽出するため、データの可視化や変換、生成、クラスタリングなどの様々なタスクに応用される。教師なし学習の手法や自己教師あり学習（自己教師あり学習）の一環として学習されることがある。オートエンコーダや変分オートエンコーダなどのニューラルネットワークモデルでは、この表現を学習するために広く使用される。`},
{target:`判別器 discriminator`, content:`敵対的生成ネットワーク（GAN）において生成器から生成されたデータと実際のデータを区別する役割を担うモデル。<br>入力データが生成されたデータであるか実際のデータであるかを判別する二値分類モデルとして構築される。通常、ニューラルネットワークの形式を取る。例えば、畳み込みニューラルネットワーク（CNN）や全結合ニューラルネットワーク（MLP）が使用される。入力データに対して確率を出力する活性化関数（通常はシグモイド関数）を最終層に持つ。生成器が生成したデータを入力とし、0から1の範囲の値を出力する。出力が1に近いほど、入力データが実際のデータに近いと判断される。生成器がより本物に近いデータを生成するように学習する。生成器が生成したデータを入力とし、その出力が実際のデータであると判別されるほど、生成器の性能は向上する。GANの学習では、生成器とこれが互いに競い合いながら学習を進め、最終的に生成器が高品質なデータを生成できるようになることを目指す。この訓練には、生成器から生成されたデータと実際のデータの組を用いて行う。訓練データにおいて、生成器から生成されたデータはラベル0（生成データ）として、実際のデータはラベル1（実データ）として与えられる。これらのデータを適切に分類するように学習される。目的は、生成器が生成したデータを偽と判定すること。生成器とこれの学習は相互に関連しており、生成器はこれが誤って生成データを実データと判定するようなデータを生成することを目指す。これにより、生成器はよりリアルなデータを生成できるようになる。`},
{target:`モード崩壊 mode collapse`, content:`敵対的生成ネットワーク（GAN）の訓練中に発生する問題の1つ。<br>モードとは、データ分布の中で特定のパターンやクラスターを指す。生成器が制約のない状態で学習してしまい、訓練データの多様性を十分に反映できない状況を指す。結果として、生成器が訓練データの一部のモードしか捕捉せず、他のモードが無視されることがある。これにより、生成されるデータが単調で似たようなものになってしまい、デバーシティ（多様性）の欠如が起こる。GANの訓練の安定性や品質を損なうことがあり、望ましくない結果となる。防ぐためには、①学習率の調整、②正則化の導入、③多様なミニバッチの使用、④制約の追加。これらの手法を組み合わせることで、この崩壊を軽減し、より多様なデータ生成が可能となる。`},
{target:`ミニバッチ判別 mini batch discrimination`, content:`敵対的生成ネットワーク（GAN）の訓練中に使用される手法の1つ。<br>通常、GANの判別器は各入力データに対して単一の判別結果を出力するが、この判別では、複数のデータの組み合わせに基づいて判別結果を生成する。利点、①多様性の向上、②モード崩壊の軽減、③安定性の向上。判別器の最終層の前に追加される追加の層や処理として実装。ミニバッチ内のデータの特徴を考慮して判別結果を出力。ただし、すべてのGANの設定で効果的なわけではなく、問題やデータセットによって異なる結果が得られる場合がある。適切なハイパーパラメータの調整やモデルの選択が重要。`},
{target:`深層畳み込み Deep Convolutional GAN DCGAN`, content:`GANの一種であり、畳み込みニューラルネットワーク（CNN）を使用して高品質な画像生成を行うためのアーキテクチャ。<br>DCGANは、生成器と識別器という2つのネットワークで構成。①生成器（Generator）・・・ランダムノイズを入力として受け取り、それを使って画像を生成。生成器は、逆畳み込み層（転置畳み込み層）を使用し、ノイズを次元を増やしながら画像の形状に変換していく。最終的に、生成器は本物の画像に近い出力を生成することを目指す。②識別器（Discriminator）・・・生成器が生成した画像と本物の画像を識別する役割を担う。識別器は、畳み込み層を使用して画像の特徴を学習し、生成された画像と本物の画像を区別。識別器は、生成器が生成した画像を偽物として識別し、本物の画像を真として識別するように学習。DCGANの特徴、①畳み込み層と逆畳み込み層を使用して画像の生成と識別、②バッチ正規化（Batch Normalization）を使用し学習の安定性を向上、③LeakyReLUやReLUといった活性化関数を使用して、モデルの非線形性を増加、④ジェネレータとディスクリミネータの間にプーリング層は使用されず、ストライド畳み込みや逆畳み込みが使用。DCGANは、大規模なデータセットで高品質な画像生成を実現することができる。そのため、画像生成やデータ拡張などの応用に広く使用。`},
{target:`CGAN Conditional Generative Adversarial Network`, content:`生成モデルの一種、敵対的生成ネットワーク（GAN）の一変種。生成器と識別器の両方が条件付きで動作することが特徴。<br>生成器と識別器の両方に追加の条件情報（クラスラベルや特定の属性など）が与えられ、生成器は特定の条件に基づいてデータを生成することができ、識別器は生成されたデータが与えられた条件に適合しているかどうかを判断する役割を持つ。学習では、生成器と識別器は相互に競い合う最小最大ゲーム（min-max game）が行われる。条件付き生成タスクにおいて有用であり、例えば画像生成において特定のクラスの画像を生成するなどの応用がある。また、条件付き生成モデルの枠組みを拡張して、セマンティックセグメンテーションや画像修復など、さまざまなタスクに応用することも可能。`},
{target:`PGGAN Progressive Growing of GANs`, content:`NVIDIAチームが提案した生成的敵対的ネットワーク（GAN）の一種。画像生成タスクにおいて高解像度な画像の生成を可能にする手法。<br>通常のGANでは、低解像度から高解像度へと逐次的に生成を行うため、高解像度な画像の生成には困難さがある。そこでこの問題に対処するために、段階的な生成を導入した。生成器と識別器を段階的に成長させることで高解像度な画像の生成を実現。まず、低解像度の画像から始めて、生成器と識別器を訓練し、その後、解像度を徐々に上げながら生成器と識別器を更新、より高解像度な画像の生成を目指す。この段階的な成長は、生成器と識別器のネットワーク構造を徐々に拡張することで実現。新しい解像度のレイヤーが追加され、既存のレイヤーは凍結された状態で訓練が進む。利点は、低解像度の画像から始めるため、初期段階では比較的容易に学習が進む。また、段階的な成長により、高解像度な画像の生成を徐々に向上させることができる。さらに、低解像度から高解像度への段階的な生成により、より多様な画像を生成することが可能。顔写真や自然風景などの高解像度な画像生成において優れた結果を示す。また、他のアプリケーションにも応用が広がっており、画像編集やデザインなど様々な分野で利用。`},
{target:`出力畳み込み層 output convolutional layer`, content:`畳み込みニューラルネットワーク（CNN）における最後の層。<br>この層は、ネットワークの出力を生成するために使用される。この層は、通常、一連の畳み込み層とプーリング層の後に配置される。これらの層によって抽出された特徴マップは、この層に供給される。特徴マップから最終的な出力を生成するために畳み込み演算を適用する。「カーネルサイズ1」の通常の「畳み込み層」で、出力を適切な「カラーチャネル数」に「投影」する。「新しい畳み込み層」が追加されたときに最初の「畳み込み層」の訓練された「重み」を壊さないようにするために、最終的な出力は、もとの「出力層」と新しい「出力層」の加重総和にしている。新しい出力の「重み」は α、もとの出力の「重み」は1-αで、αは0から1に向かって少しずつ大きくしていく。つまり、新しい「畳み込み層」が次第に「フェードイン」し、もとの「出力層」が次第に「フェードアウト」していく。「判別器」に新しい「畳み込み層」を追加するときにも、同じような「フェードイン/フェードアウト」のテクニックが使われている。`},
{target:`ミニバッチ標準偏差層 minibatch standard deviation layer`, content:`生成的敵対的ネットワーク（GAN）などの生成モデルで使用される層の一つ。<br>ネットワークによって生成された特徴マップの各チャネルに対して、ミニバッチごとの標準偏差を計算し、特徴マップに結合する。ネットワークはミニバッチ内のサンプル間の統計的な情報を利用できるようになり、生成モデルがより多様なサンプルを生成することができる。また、モード崩壊（Mode Collapse）の問題を緩和する。`},
{target:`チャネル`, content:`画像やビデオのデータ表現において、色や特徴の情報を表すための次元。<br>一般的に、カラー画像では赤、緑、青の3つが使用される。`},
{target:`特徴マップ`, content:`畳み込みニューラルネットワーク（Convolutional Neural Network, CNN）などの深層学習モデルにおいて、入力データから抽出された特徴の空間的な表現。`},
{target:`均等化された学習率 equalized learningrate`, content:`生成的敵対ネットワーク（GAN）などの深層学習モデルで使用される一種の重みの初期化手法。<br>通常、重みの初期化はランダムな値や特定の分布からサンプリングされるが、Equalized learning rateでは、重みの初期化時に学習率を均等化することによって、ネットワークの安定性や収束性を向上させる。`},
{target:`ピクセル単位の正規化層 pixelwise normalization layer`, content:`生成的敵対ネットワーク（GAN）などの深層学習モデルで使用される正規化手法の一つ。<br>この層は、生成器の出力や中間層の特徴マップをピクセル単位で正規化する役割を持つ。通常、画像生成タスクでは、生成器が出力する画像の各ピクセルの値が[-1, 1]や[0, 1]の範囲に収まるように調整されることが望ましい。しかし、生成器の出力や特徴マップは、異なるピクセルごとに異なる範囲の値を持つことがある。これにより、生成される画像の品質や安定性に悪影響を及ぼすことがある。この正規化層は、このような問題を解決するために導入された。具体的には、特徴マップの各ピクセルに対して、チャネルごとに平均値を計算し、ピクセルごとの平均値で各ピクセルの値を正規化する。これにより、ピクセルごとに異なる範囲の値を持つ特徴マップを統一された範囲に持ってくることができる。この正規化層は、生成器の学習の安定性や品質向上に効果がある。また、生成される画像の多様性や品質の向上にも寄与する。`},
{target:`StyleGAN`, content:`NVIDIAによって開発され画像生成のための深層生成モデルの一種。高解像度でリアルな画像の生成に特化しており、特に顔画像の生成に優れた結果を示す。<br>最も顕著な特徴は、スタイル変換の概念の導入。スタイル変換により、生成される画像の異なる要素（例 髪の色、顔の形、表情など）を独立して操作することができる。これにより、生成される画像の多様性やカスタマイズ性が向上した。また、これは畳み込みニューラルネットワークの生成器と判別器のアーキテクチャを進化させている。生成器は、逐次的なスタイルの転送を行うスキップ接続という仕組みを採用し、より高品質な画像の生成を可能にする。判別器は、画像の評価をより正確に行うための複数のスケールの判別器を利用する。その高品質な画像生成能力やスタイルの柔軟性から、様々なアート作品や写真の生成、仮想キャラクターの作成などの応用に利用されている。「判別器」と「損失関数」は変更されておらず、「生成器」が変更されただけである。この「生成器」は、「マッピングネットワーク」と「合成ネットワーク」のネットワークから構成。`},
{target:`スタイル style`, content:`生成される画像の外観や特徴を制御するための要素。<br>具体的には、生成される画像の色調やテクスチャ、ディテールの表現などがスタイルに関連する。`},
{target:`スタイル転送 style transfer`, content:`異なる画像のスタイルを組み合わせるテクニック。<br>この手法は、画像のスタイル（例 絵画や写真のスタイル）とコンテンツ（画像の内容）を分離し、一つの画像のコンテンツに別の画像のスタイルを適用することで、独自のクリエイティブな画像を生成することができる。`},
{target:`局所構造 local structure`, content:`画像生成モデル（例 GAN、Autoencoder）において、画像内の特定の領域やパターンに対して特定のスタイルや特徴を持たせるための手法や概念。<br>StyleGANでは、スタイル変数（latent code）を分割し、特定の領域に関連するスタイル変数を制御することで、この構造を実現。`},
{target:`マッピングネットワーク mapping network`, content:`StyleGANの潜在ベクトル（latent vector）を生成するためのネットワーク。<br>入力としてランダムなノイズベクトルを受け取り、それをより意味のある潜在ベクトルに変換する。具体的には、複数の全結合層（Dense層）からなり、非線形変換を経て入力を潜在ベクトルに写像する。この変換によって、潜在ベクトルの各次元はより意味を持ったスタイル要素を表現するようになる。役割は、ランダムなノイズベクトルから意味のある潜在ベクトルを生成する。この潜在ベクトルを用いて生成器によって画像が生成され、スタイルの制御や調整が可能となる。StyleGANの学習中にも逐次的に更新されることがあり、より効果的なスタイルの生成を実現するための学習が行れる。`},
{target:`合成ネットワーク synthesis network`, content:`ランダムノイズベクトル（潜在ベクトル）から画像を生成する役割を担っている。<br>①ノイズマップ（Noise Map）・・・ノイズマップは、ランダムなノイズパターンを表現するためのテンソル。生成される画像の詳細な特徴やテクスチャを制御する役割を果たす、②スタイルマップ（Style Map）・・・潜在ベクトルの要素と対応するスタイルの情報をエンコードするためのテンソル。生成される画像のスタイルや特定の局所構造を制御する役割を果たす、③合成層（Synthesis Layers）・・・ノイズマップとスタイルマップを入力として受け取り、合成画像の特徴マップを生成する。これには畳み込み層、逆畳み込み層、正規化層などが含まれる場合がある。ノイズマップとスタイルマップの組み合わせを調整することで、生成される画像の外観や特徴を制御することができる。ノイズマップはランダムなパターンを持ち、スタイルマップは潜在ベクトルの各要素に対応するスタイル情報を持つことで、生成器の柔軟な画像生成が可能。`},
{target:`適応インスタンス正規化 adaptive instance normalization`, content:`画像生成モデルで使用される正規化の手法の一つ。<br>入力画像のスタイルを保持しながら、特定のスタイルを別の画像に適用することが可能。主にスタイル転送や画風変換などのタスクで利用される。`},
{target:`AdaIN層 Adaptive Instance Normalization`, content:`適応インスタンス正規化（Adaptive Instance Normalization）を実装するための層。<br>入力特徴マップに対して平均と標準偏差の正規化を行い、それをスタイル特徴マップの平均と標準偏差に適用する。`},
{target:`ノイズ入力`, content:`StyleGANの生成器では、スタイルベクトルとして扱われる。スタイルベクトルは、生成される画像の様々な特徴やスタイルを制御するためのベクトル。<br>一般的にガウス分布から生成されたランダムなベクトルが使用されるが、StyleGANではスタイルベクトルとして処理され、画像生成の細部やスタイルの表現に寄与する。`},
{target:`強化学習 Reinforcement Learning RL`, content:`人工知能（AI）が環境との相互作用を通じて学習する手法。<br>エージェント（AI）は環境の中で行動を選択し、その結果として報酬を受け取る。エージェントは報酬を最大化するように行動を選択することを目指して学習を進める。学習のアルゴリズムは、①状態、②行動、③報酬、④方策、⑤価値、の要素から構成される。目標は、最適な方策や価値関数を見つけること。これにより、エージェントは最適な行動を選択し、報酬を最大化することができる。自律的な意思決定や制御問題に応用される。例えば、ゲームプレイやロボット制御などの領域で利用され、最近では自動運転技術の開発にも活用されている。`},
{target:`状態 state`, content:`エージェントが環境から観測する情報で、現在の状況や環境の状態を表す。<br>エージェントが行動を選択するための情報源。`},
{target:`行動 action`, content:`エージェントが選択する可能な行動の集合。<br>離散的な値や連続的な値を取り得る。エージェントは状態に応じて選択。`},
{target:`方策 policy`, content:`エージェントの行動選択戦略を表す。<br>状態に基づいて行動を選択するルールや確率分布として定義される。`},
{target:`報酬 reward`, content:`エージェントが行動に対して受け取る数値的なフィードバック。<br>エージェントが目指す目標を反映した評価指標。`},
{target:`学習 learning`, content:`エージェントは、環境との相互作用を通じて経験を蓄積し、報酬を最大化するための適切な行動選択を学習。`},
{target:`エージェント agent`, content:`強化学習の文脈で行動する主体を指す。<br>これは環境との相互作用を通じて行動を選択し、状態の観測や報酬の受け取りを通じて学習を進める。「環境」（environment）内で、「観測データ」（observation）に基づいて、「行動」（action）を取り、それと引き換えに「報酬」（reward）を受け取る。そして長期的に期待できる「報酬」を最大化するように、「行動」を学ぶ。また、「行動」を決めるためのアルゴリズムを「方策」（policy）と呼ぶ。`},
{target:`確率的方策 stochastic policy`, content:`強化学習においてエージェントが状態に応じて行動を選択する確率分布。<br>同じ状態に対して複数の行動があり得る場合、それぞれの行動に対する確率が割り当てられる。主に、①カテゴリカル方策（Categorical Policy）、②ガウス方策（Gaussian Policy）の形式で表現される。エージェントが状態に応じて異なる行動を選択することを可能にし、探索と活用のトレードオフを調整することができる。探索の一部として、エージェントは探索的な行動を選択することで、未知の領域や最適解に到達する可能性を高める。一方、活用の一環として、エージェントは報酬を最大化するような確率的な行動を選択することが求められる。強化学習の手法やアルゴリズムの実装において重要な役割を果たす。エージェントは、方策を学習し、最適な行動選択を実現するために、この方策の更新や改善を行う。また、エージェントの行動の多様性や探索の効果を調整する上でも有用。`},
{target:`カテゴリカル方策 categorical policy`, content:`この方策では、離散的な行動空間を持つ問題に適用。<br>各行動に対する確率が明示的に定義され、行動の選択は確率分布からのサンプリングによって行われる。`},
{target:`ガウス方策 gaussian policy`, content:`この方策は、連続的な行動空間を持つ問題に使用。<br>ガウス分布（正規分布）を用いて、各行動の平均値と分散を表現。エージェントは、ガウス分布からのサンプリングによって行動を選択。`},
{target:`方策パラメータ policy parameters`, content:`強化学習において方策（Policy）を定義するために使用される変数の集合。<br>エージェントが状態に応じて行動を選択するためのルールや確率分布を決定する。エージェントは、状態を入力として受け取り、このパラメータに基づいて行動を選択。エージェントの学習中に更新されることがある。`},
{target:`方策探索 policy search`, content:`強化学習の手法の一つで、最適な方策を見つけるために探索する手法。<br>方策のパラメータや形式を変化させながら、報酬を最大化するような良好な方策を見つけることを目指す。①確率的な探索、②グリッドサーチやランダムサーチ、③進化戦略（「遺伝的アルゴリズム」）、④勾配ベース、のような手法が利用される。強化学習の問題において探索と活用のトレードオフを調整する手法として重要。方策空間の探索を行い、最適な方策を見つけることで、報酬を最大化するエージェントの行動選択を実現。`},
{target:`確率的な探索`, content:`ランダムな行動選択やノイズを導入して、探索的な行動を促す手法。<br>エージェントは、未知の領域や最適解に到達する可能性を高めるために、探索を行う。`},
{target:`グリッドサーチおよびランダムサーチ`, content:`方策のパラメータ空間をグリッド状にサンプリングしたり、ランダムにサンプリングすることで、探索を行う。<br>探索空間全体を試行しながら、報酬を最大化するような方策を見つける。`},
{target:`進化戦略遺伝的アルゴリズム`, content:`この戦略では、遺伝的アルゴリズムや進化計算手法を応用して、方策パラメータを進化させる手法。<br>個体群内の方策パラメータを評価し、適応度に基づいて次世代のパラメータを選択。`},
{target:`勾配ベース`, content:`この方法では、方策パラメータの勾配情報を利用して方策を改善。<br>勾配情報に基づいてパラメータを更新することで、報酬を最大化するような方策を見つける。代表的な手法には、方策勾配法（Policy Gradient）やTRPO（Trust Region Policy Optimization）、PPO（Proximal Policy Optimization）などがある。`},
{target:`遺伝的アルゴリズム genetic algorithm`, content:`進化生物学の進化理論を基にした最適化手法の一種。<br>このアルゴリズムは、個体群内の個体（解候補）を遺伝子表現で表し、適応度（評価関数の値）に基づいて個体を評価・選択し、遺伝子操作（交叉・突然変異）を通じて新たな個体を生成・進化させることで最適解を探索する。`},
{target:`OpenAIGym`, content:`強化学習の研究や開発のための一般的なプラットフォームで、様々なタスクや環境を提供。<br>これを使用することで、エージェントが強化学習アルゴリズムを学習し、性能を評価することができる。さまざまな「シミュレート環境」（アタリゲーム、ボードゲーム、2次元、3次元物理シミュレーションなど）を提供してくれるツールキットで、これを使えば、「エージェント」を訓練、比較し、新しい「RLアルゴリズム」を開発できる（https://gym.openai.com/）。`},
{target:`アタリゲーム atarigames`, content:`Atari社が1970年代から1980年代にかけてリリースしたビデオゲームのシリーズ。<br>強化学習の研究やベンチマークとしてもよく利用。特に、OpenAI Gymなどの環境では、様々なゲームをエージェントが学習・プレイすることができる。高次元の状態空間や行動空間を持ち、エージェントの性能評価や強化学習アルゴリズムの比較において有用なテストベッドになっている。`},
{target:`レンダリング rendering`, content:`コンピュータグラフィックスにおいて、3Dモデルやシーンを2D画像や動画として表示するプロセスを指す。<br>3D空間内のオブジェクトの形状、材質、光源、カメラの位置などを考慮し、最終的な視覚的な表現を生成するための計算手法。`},
{target:`ハードコード`, content:`特定の「動作環境」を決め打ちして、その「環境」を前提としたデータをソースコードの中に直に記述する。`},
{target:`探求 exploring`, content:`強化学習においてエージェントが未知の状態や行動に積極的に対処することを指す。<br>エージェントが環境を探索し、新たな情報を収集するために行われる。強化学習では、エージェントは報酬を最大化するために最適な行動を学習する。しかし、初めての状態や未知の行動に対しては、エージェントは正しい行動を知らないため、探求が必要。一般的な探求手法には、①ε-Greedy法、②UCB1（Upper Confidence Bound 1）、③Thompson Sampling、のようなものがある。これらの探求手法は、エージェントが最適な行動を学習するために必要な探索と活用のバランスを取るために使用される。探求を行うことで、エージェントは未知の状態や行動に対しても適切な行動を学習し、より高い報酬を得ることができる。`},
{target:`UCB1 Upper Confidence Bound1`, content:`エージェントは、各行動の不確実性を考慮して行動を選択。<br>未知の行動や報酬の不確実性が高い行動に対して優先的に探求を行う。`},
{target:`thompson sampling`, content:`エージェントは、各行動の確率分布をベイズ推定し、不確実性の高い行動を選択する確率を高めに設定。<br>これにより、報酬の不確実性が高い行動に対して探求を行う。`},
{target:`利用 exploiting`, content:`強化学習においてエージェントが既に学習済みの知識や経験を活用して最適な行動を選択することを指す。<br>利用は、エージェントが過去の学習結果を基に、報酬を最大化するための戦略を追求することを意味する。利用は、探求（探索）と対比される概念。探求は未知の領域を探索し、新たな知識を獲得するために行われるのに対し、利用は既知の知識を活用して最適な行動を選択するために行われる。強化学習の目標は、探求と利用のバランスを取ること。エージェントは探求を通じて未知の領域を探索し、新たな知識を獲得しつつ、学習済みの知識を利用して報酬を最大化するための戦略を追求する。`},
{target:`信用割当問題 credit assignment problem`, content:`行動と結果の間の因果関係が複雑である場合に、正確に結果に責任を割り当てることが難しい状況を指す。具体的には、エージェントが複数の行動を取り、その結果が時間的に遅れて現れる場合に、どの行動が結果に対して責任を持っているのかを正確に特定することが困難な場合にこの問題が発生する。例えば、強化学習の環境でのケースでは、エージェントが一連の行動を取った後に報酬が与えられる場合がある。しかし、最終的な報酬の値には、直前の行動だけでなく、過去の行動や状態の影響も含まれている可能性がある。このような場合、エージェントは最終的な結果を正確に評価することが困難であり、どの行動が結果に対して責任を持っているのかを特定することが難しくなる。これを解決するためには、長期的な報酬や結果と過去の行動や状態の関係を考慮に入れる必要がある。一般的な解決策は、適切な報酬関数や割引率を設計し、適切な時間的な因果関係をモデル化する。`},
{target:`割引率 discount rate`, content:`将来の報酬が現在の報酬よりも価値が低くなることを考慮するために導入。<br>一般的には、0から1の範囲の値を取り、この率が1に近い場合、将来の報酬と現在の報酬の価値を同等に扱う。一方、この率が0に近い場合、将来の報酬の価値を非常に低く評価する。強化学習の場合、この率を用いることで将来の報酬の期待値を考慮し、適切な行動価値や政策を学習することができる。一般によく使われるのは、0.9から0.99までである。0.95にすると、13ステップ後の「報酬」が、「直後の報酬」の半分ほどとして扱われる（0.95の13乗≒0.5）。これを0.99にすると、69ステップ後の「報酬」が、直後の「報酬」の半分ほどに扱われる。`},
{target:`累積報酬 cumulative reward`, content:`強化学習においてエージェントが一連の行動を取った結果として得られる報酬の合計。<br>利得、収益とも言う。`},
{target:`行動優位性 action advantage`, content:`強化学習において、ある状態での行動の価値を他の行動と比較するために使用される指標。<br>ある状態におけるある行動の価値と、他の行動の価値との差を表す。具体的には、ある状態での行動の期待収益（expected return）や行動価値（action value）から、他の行動の期待収益や行動価値を引いた値となる。エージェントが最適な行動を選択するための重要な指標。エージェントはこの優位性を基に、より良い報酬をもたらす行動を選択することを学習する。この優位性が正の値を持つ行動は、その状態で好ましい行動と見なされ、エージェントはこれらの行動を選択する傾向がある。一般的な強化学習の手法では、この優位性を評価し、最適な行動を選択するために使用される。例えば、Q学習やDQN（Deep Q-Network）などの手法では、状態行動価値関数（state-action value function）を学習し、この優位性を計算して行動選択を行う。エージェントが探索と利用のトレードオフを調整し、最適な行動を選択するための重要な要素。この計算や最適化は、強化学習のアルゴリズムの中核となる概念の一つ。`},
{target:`方策勾配法 policy gradient`, content:`強化学習において方策（Policy）を学習するための手法の一つ。より「高い報酬」に向かう「勾配」をたどって「方策」の「パラメータ」を最適化する。<br>報酬を最大化するような方策をパラメータ化し、そのパラメータを更新することで学習を進める。方策の更新は、報酬に基づいて行動の選択確率を調整することで行われる。報酬が高い行動は選択されやすくなり、報酬が低い行動は選択されにくくなるように方策を更新する。具体的な手法としては、勾配法（Gradient Descent）を用いて方策パラメータを更新する手法が一般的。実世界の制御問題やゲームプレイにおいて優れた性能を示すことがあり、現在の強化学習の研究や応用において広く利用されている。①エージェントは初期の方策パラメータを設定、②エージェントは環境との相互作用を通じて、方策に基づいて行動し、報酬を受け取る、③受け取った報酬を基に、方策のパラメータを微小な変化（勾配）の方向に更新、④②と③のステップを複数回繰り返し、方策パラメータを最適化。`},
{target:`REINFORCEアルゴリズム`, content:`強化学習においてポリシー勾配法（policy gradient method）の一種、<br>方策（ポリシー）のパラメータを更新するための手法。エージェントが環境との相互作用を通じて得られるトラジェクトリ（軌跡）に基づいて方策の更新を行う。方策パラメータを更新することでエージェントが最適な行動を学習。この手法は、確率的な行動選択と報酬の累積を通じて、方策を学習するため、離散的な行動空間や連続的な行動空間の問題に適用できる。`},
{target:`テープ tape`, content:`計算グラフ上での計算履歴を記録するデータ構造。<br>強化学習の場合、エージェントの方策（ポリシー）や価値関数などのパラメータを更新するために、損失関数を最小化する必要がある。この損失関数を計算するために、これを使用する。これは、計算グラフ上での各演算（加算、乗算、活性化関数など）とその出力値を記録。エージェントが環境との相互作用を通じて得られたデータを入力として与え、計算グラフ上での順伝播（forward propagation）が行われ、その際、これは計算過程を記録する。順伝播が終了すると、これには計算グラフ上の計算履歴が記録される。この計算履歴を使用して、逆伝播（backward propagation）が行われ、勾配が計算される。勾配は、損失関数のパラメータに対する微分値であり、パラメータの更新方向を決定するために使用される。これを使用することで、エージェントの方策や価値関数のパラメータを更新する際に、勾配降下法やその他の最適化手法を適用することができる。これによって自動的に微分が計算されるため、パラメータの更新が容易になる。一般的に、これはディープラーニングフレームワーク（例 TensorFlowやPyTorch）に組み込まれており、強化学習のアルゴリズムで使用される損失関数の計算や勾配の取得に活用される。`},
{target:`報酬リスト reward list`, content:`エージェントが実行した行動に対して受け取った報酬を順番に記録するために使用。<br>エージェントがタスクを実行する際に報酬を最大化することが目標である場合、エージェントのパフォーマンスを評価するためにも使用される。`},
{target:`勾配リスト gradient list`, content:`エージェントの学習において各パラメータの勾配を格納したリスト。<br>勾配降下法では、損失関数の勾配を計算し、その勾配の逆方向にパラメータを更新することで最適化を行う。エージェントはエピソードやバッチごとに複数の勾配を計算し、それぞれのパラメータに対して勾配を格納する。エージェントの学習の可視化や解析に役立つ。勾配の統計情報や推移を調べることで、エージェントの学習の収束性や勾配の振る舞いを評価することができる。また、このリストを使用して勾配の平均や分散などの統計量を計算することも可能。これにより、エージェントの学習の安定性やパフォーマンスの向上を評価することができる。`},
{target:`マルコフ連鎖 markov chain`, content:`確率過程の一種であり、ある特性を持つ確率的な状態の遷移のシーケンスを表現する。<br>この連鎖は「マルコフ性」を満たすという特徴を持つ。マルコフ性とは、次の状態が現在の状態にのみ依存し、過去の状態や遷移履歴には依存しない性質。この連鎖は、①状態空間、②遷移確率、③状態遷移、④無記憶性、の特徴を持つ。様々な応用分野で利用される。例えば、ランダムウォークやマルコフ決定過程、マルコフチェーンモンテカルロ法などがある。強化学習においても、状態遷移確率がこの連鎖の性質を持つ場合に、価値関数や方策を推定するために利用されることがある。`},
{target:`状態空間 state space`, content:`マルコフ連鎖の状態の集合を表す。<br>状態は離散的な値や連続的な値を取る。`},
{target:`無記憶性 memory less property`, content:`マルコフ連鎖では、次の状態への遷移が現在の状態にのみ依存し、過去の状態や遷移履歴には依存しない。<br>これにより、過去の状態を覚えておく必要がなく、現在の状態だけで次の状態を予測することができる。`},
{target:`マルコフ決定過程 Markov Decision Process MDP`, content:`強化学習の基本的な枠組みの一つで、時間的に連続的な意思決定問題をモデル化し、最適な行動選択とその結果としての報酬獲得を目指す。<br>①状態、②行動、③遷移確率、④報酬、⑤割引率、の要素で構成される。エージェントは現在の状態と遷移確率に基づいて行動を選択し、遷移後の状態と得られる報酬を観測する。エージェントは報酬の最大化を目指して最適な行動選択を学習し、価値関数や方策を推定することが求められる。一般的に、最適解を求めるためには、価値反復法やQ学習、方策勾配法などのアルゴリズムを使用。`},
{target:`遷移確率 transition probability`, content:`状態と行動の組み合わせに対して、次の状態に遷移する確率を表す。<br>この確率は状態間の遷移を確率的に決定する。`},
{target:`割引率 discount factor`, content:`未来の報酬の価値を現在の報酬よりも低く評価するための率。<br>この率は0から1の値を取り、1に近づくほど将来の報酬に高い重要性を持たせる。`},
{target:`状態遷移 state transition`, content:`マルコフ決定過程（MDP）において現在の状態から次の状態へ移行するプロセスを指す。<br>この遷移は確率的に行われ、遷移確率と呼ばれる確率分布に従う。この遷移はマルコフ性を持つため、次の状態への遷移は現在の状態にのみ依存し、過去の状態や遷移履歴には依存しない。これにより、エージェントは現在の状態のみを観測して次の行動を選択することが可能となる。`},
{target:`最適状態価値 optimal state value`, content:`マルコフ決定過程（MDP）において最適な方策を採用した場合の各状態の価値を表す。<br>エージェントが目標を達成するために最適な行動を選択した場合に期待される収益や報酬の合計値を示す。ある状態から最適な方策を選択した場合に得られる累積報酬の期待値を表す。この期待値は、割引率を考慮して未来の報酬を割引した価値。通常、「V*（s）」と表記される。`},
{target:`ベルマン最適方程式 bellman optimality equation`, content:`マルコフ決定過程（MDP）において最適な状態価値や行動価値を求めるための方程式。<br>この方程式には「状態価値関数」と「行動価値関数」の形式の2つがある。最適な状態価値や行動価値を求めるための基礎となる方程式であり、価値反復法や強化学習アルゴリズムにおいて使用される。これらの方程式を解くことで、最適な方策を見つけたり、最適な行動選択を行ったりすることが可能。`},
{target:`状態価値関数 bellman optimality equation for state value function`, content:`ある状態の最適な価値が、その状態で可能な全ての行動の価値の最大値と、次の状態の最適な価値との関係を示している。`},
{target:`行動価値関数 bellman optimality equation for action value function`, content:`ある状態と行動の組み合わせの最適な価値が、次の状態と可能な全ての行動の最適な価値の組み合わせに基づいて計算。`},
{target:`価値反復法 value iteration`, content:`マルコフ決定過程（MDP）において最適な状態価値関数や行動価値関数を求めるための反復アルゴリズム。<br>この方法は、ベルマン最適方程式を反復的に解くことによって最適な価値関数を近似する。最適な価値関数を求めるための効果的な手法。反復的な更新によって価値関数を収束させることで、最適な方策や最適な行動選択を得ることができる。ただし、状態空間や行動空間が非常に大きい場合には計算量が膨大になるため、近似手法や関数近似を組み合わせて使用することが一般的。`},
{target:`動的計画法 dynamic programming`, content:`最適化問題を解くための一連の手法とアルゴリズムの総称。<br>特に、重複する部分問題が存在し、それらを効率的に解くことができる場合に適用される。①最適性原理、②再帰的な構造、の特徴を持つ。`},
{target:`最適性原理 principle of optimality`, content:`最適な解は、部分問題の最適な解から構築される。<br>つまり、最適な解は部分問題の最適な解の組み合わせとして表現される。`},
{target:`再帰的な構造`, content:`最適性原理に基づいて、大きな問題を小さな部分問題に分割し、再帰的に解を求める。<br>小さな部分問題の解が求まると、それらを組み合わせて大きな問題の解を導く。`},
{target:`状態行動価値 state action valueまたはQ値 Q value`, content:`強化学習において使用される重要な概念で、ある状態と行動の組み合わせに対して、その組み合わせが取られた場合の将来の報酬の期待値を表す。<br>通常、Q関数（Q-function）またはQテーブル（Q-table）として表現される。Q関数は、状態と行動の組み合わせを入力として受け取り、対応するこの価値を出力する関数。Qテーブルは、状態と行動の組み合わせに対応する値を格納した表の形式で表現される。この価値の更新は、強化学習の学習アルゴリズムによって行われる。代表的なアルゴリズムの一つはQ学習（Q-learning）。Q学習では、エージェントが環境とやり取りしながら、経験を元にこの価値を更新する。具体的には、以下のベルマン方程式を用いて更新が行われる。エージェントは最適な行動を選択するためのこの価値を徐々に学習していく。最適な方策を求めたり、行動価値に基づいて行動選択を行ったりする際に使用される。Q学習やその他の強化学習アルゴリズムは、この価値の更新を通じて最適な行動価値関数や方策を近似し、エージェントの性能向上を実現する。`},
{target:`推定Q値 estimated Q value`, content:`強化学習において状態と行動のペアに対して推定される行動価値の値。<br>Q値は状態と行動の組み合わせに対してその価値を表し、最適な行動を選択するための指標。通常はQ関数（状態行動価値関数）によって近似される。Q関数は、状態と行動を入力として受け取り、その状態と行動の価値を出力する関数。`},
{target:`Q値反復アルゴリズム Q value iteration algorithm`, content:`強化学習において最適なQ値を求めるためのアルゴリズム。<br>動的計画法の一種であり、状態価値関数や行動価値関数を更新することによって最適な政策を見つける手法。状態と行動の組み合わせが有限である場合に効果的。全ての状態と行動の組み合わせについてQ値を更新するため、計算量が大きくなる可能性がある。しかし、状態空間や行動空間が小さい場合には、最適な政策を効率的に求めることができる。`},
{target:`TD学習アルゴリズム Temporal Difference learning algorithm`, content:`強化学習の一種であり、状態価値関数や行動価値関数を近似的に学習する手法。<br>モンテカルロ法と動的計画法の中間に位置する。現在の推定値を使用して未来の推定値を更新することで、価値関数を学習する。特徴は、リアルタイム性とサンプル効率性。モンテカルロ法のようにエピソードの終了まで待たずに学習を行うことができ、かつ動的計画法のように完全なモデル情報は必要ない。`},
{target:`探索方策 exploration policy`, content:`強化学習においてエージェントが未知の領域や最適解にたどり着くために探索を行うための方策。<br>既知の情報や経験に基づいて行動を選択する「利用（Exploitation）」と、未知の情報を探索するためにランダムな行動を選択する「探求（Exploration）」との間のトレードオフを取る必要がある。この方策を適切に設計することで、エージェントは未知の状態や行動を探索し、最適な行動を見つけることができる。`},
{target:`Q学習 Q learning`, content:`強化学習における一つのアルゴリズムであり、最適な行動価値関数（Q関数）を学習する手法。<br>状態と行動のペアに対する行動価値を表すQ値を更新することで学習を進める。基本的なアルゴリズムは、①環境の初期化およびQ関数の初期化、②エージェントの行動選択、③行動の実行と報酬の観測、④Q値の更新、⑤②から④を繰り返す、のような手順で行われる。モデルフリーな手法であり、環境のモデル（遷移確率や報酬関数）を事前に知る必要がない。この特徴から、実世界の複雑な問題においても利用される。また、オフポリシーの手法であり、行動価値関数を学習することで最適な方策を得ることができる。ただし、状態・行動空間が大きい場合や連続な状態空間に適用するのが難しいという課題がある。そのような場合には、関数近似やディープラーニングを組み合わせた手法が利用される。`},
{target:`ε greedy法 ε greedy algorithm`, content:`探索と活用のトレードオフを行うための手法。<br>主にQ学習などの強化学習アルゴリズムで使用。ある確率εでランダムな行動を選択し、残りの確率1-εで現在の最適な行動を選択する。つまり、εの値によってランダムな行動を選択する確率が決まる。この手法の目的は、ランダムな探索を行うことで新たな行動や状態を探索し、学習を進めること。初期段階では探索を重視し、ランダムな行動を選択する確率εを高く設定する。学習が進んでQ値が更新されていくと、最適な行動を選択する確率が高くなり、活用を重視する。探索と活用のトレードオフをうまくバランスする手法。探索を適度に行いながら、最適な行動価値を学習することができる。εの値の設定によって、探索の割合を調整することが可能。初期段階では探索を重視し、学習が進むにつれて探索の割合を減らしていくことが一般的。`},
{target:`方策オンアルゴリズム on policy algorithm`, content:`エージェントが現在の方策を使用して環境と対話しながら学習を進める手法。<br>具体的には、収集したデータを基に現在の方策を評価し、それを改善するための更新を行う。エージェントが自身の方策に基づいてデータを収集し、その方策を改善するという一貫性を持っている。自身の方策に基づいて学習を進める。`},
{target:`方策オフアルゴリズム off policy algorithm`, content:`エージェントが複数の方策を使用してデータを収集し、それを基に別の方策を改善する手法。<br>具体的には、収集したデータを使用して別の方策の評価や改善を行う。エージェントが現在の方策とは異なる方策を使用してデータを収集し、そのデータを基に方策を改善する。別の方策を使用して学習を行う。`},
{target:`近似Q学習 approximate Q Learning`, content:`強化学習の一種であり、状態行動価値関数（Q関数）を近似することで最適な方策を学習する手法。<br>通常のQ学習では状態や行動の数が非常に大きい場合や連続的な場合、Qテーブルを保持するのは非現実的。そのため、関数近似器（ニューラルネットワークなど）を使ってQ関数を近似する。`},
{target:`DQN Deep Q Network`, content:`深層強化学習（Deep Reinforcement Learning）において、Q値を近似するために深層ニューラルネットワークを使用する手法。<br>強化学習における価値ベースの手法の一つであり、特に画像などの高次元の入力を扱う場合に威力を発揮。Q値の近似に深層ニューラルネットワークを用いることにより、高次元の状態空間や連続的な行動空間でも効果的な学習が可能。Atariゲームやロボット制御など、様々なタスクで優れた性能を示した。発展形として、Double DQN、Dueling DQN、Rainbow DQNなど、さまざまな拡張や改良が提案されており、強化学習の研究や実践において広く活用されている。①エージェントは環境との相互作用を通じて経験データを収集。経験データには、状態、行動、報酬、次の状態などが含まれる、②収集した経験データをリプレイバッファ（Replay Buffer）と呼ばれるメモリに保存。リプレイバッファは、過去の経験データをランダムにサンプリングすることで学習の効率性を向上させる、③リプレイバッファからサンプリングした経験データを用いて、深層ニューラルネットワークを訓練。ニューラルネットワークは、状態を入力とし、各行動のQ値を出力するように設計、④Q値の近似に基づき、方策（行動選択戦略）を更新。一般的にはε-greedy法などの探索と活用のトレードオフを用いて行動を選択、⑤②から④のステップを反復的に繰り返し、Q値の近似と方策の改善を行う。`},
{target:`ターゲットQ値`, content:`強化学習において更新の対象となる目標となるQ値の値。<br>現在の推定Q値と未来の報酬を考慮して計算される。一般的な強化学習アルゴリズムでは、ベルマン方程式を基に計算される。ベルマン方程式は、ある状態でのQ値が次の状態のQ値と即時報酬の和に等しいという関係を表している。`},
{target:`フーバー損失 huber loss`, content:`回帰問題において外れ値の影響を緩和するために使用される損失関数の一つ。<br>通常の二乗誤差損失では外れ値によって大きく影響を受けることがあるが、外れ値に対してロバストな性質を持つ。δ以下の差分では二乗誤差を用い、δを超える差分では線形損失を用いる。`},
{target:`再生バッファ replay buffer`, content:`強化学習の一部のアルゴリズムで使用されるデータの保存および再利用のためのメモリ構造。<br>エージェントが過去の経験を保存し、後で再利用するために使用される。通常、エージェントは環境とやり取りしながら、状態、行動、報酬、次の状態などの経験を収集する。これらの経験はここに格納され、後で学習に使用される。主な目的は、エージェントの経験をランダムにサンプリングすることによって、データの相関性やバイアスを軽減し、学習の安定性と効率性を向上させること。また、経験再利用の手法としても機能し、過去の経験を再度学習に使用することで、データ効率性を向上させることができる。`},
{target:`破滅的忘却 catastrophic for getting`, content:`機械学習やニューラルネットワークにおいて、新たなデータやタスクの学習によって、以前に学習した情報や知識が失われる現象を指す。<br>既存のモデルを新しいデータで再学習すると、以前に学習した情報が上書きされ、それによって以前のタスクの性能が劣化する可能性がある。特に順次学習やドメインのシフトがある場合に顕著に現れる。新たなデータやタスクによってモデルが適応されると、以前のデータやタスクに関連する重要な情報が失われ、モデルの性能が劣化することがある。軽減するための手法としては、①経験再生、②パラメータ固定化、③ダイナミックメモリ、のようなものがある。これらの手法は、これを軽減し、モデルの性能を維持しながら新しいデータやタスクに適応するための方法として広く研究されている。`},
{target:`経験再生 experiencere play`, content:`過去のデータを再利用して学習することで、新しいデータとのバランスを保つ。`},
{target:`パラメータ固定化 parameter freezing`, content:`一部のパラメータを固定し、新しいデータやタスクに対して重要な情報を保持するために使用。`},
{target:`ダイナミックメモリ dynamic memory`, content:`過去のデータを保持し、重要な情報を適切にアクセスするためのメカニズムを導入する。`},
{target:`深層Q学習アルゴリズム Deep Q learning algorithm`, content:`強化学習の一種であり、特に行動価値関数（Q関数）の近似に深層ニューラルネットワークを使用する手法。<br>Q学習を基にしており、環境とエージェントの相互作用を通じて最適な行動価値関数を学習。しかし、従来のQ学習では状態空間や行動空間が大きい場合には効率的に学習することが難しい。そこで、深層ニューラルネットワークを用いてQ関数を近似し、高次元の状態や行動を扱うことが可能。主要なアイデアは、「経験再生」と「目標ネットワーク」の利用。経験再生では、エージェントが過去の経験をメモリに保存し、ランダムにサンプリングして学習データを作成。これにより、データの相関を緩和し、学習の安定性を向上。また、目標ネットワークでは、ターゲットQ値の計算において現在のQネットワークとは異なるネットワークを使用することで、学習の不安定性を軽減。具体的なアルゴリズムでは、エージェントは現在の状態を入力とし、Qネットワークを通じて各行動のQ値を推定。そして、ε-greedy法などの方策に基づいて行動を選択し、次の状態と報酬を観測。これらの情報を用いて、ベルマン方程式に基づいてQ値を更新。この過程を繰り返すことで、Qネットワークを最適な行動価値関数に近似していく。`},
{target:`オンラインモデル`, content:`深層ニューラルネットワークによって表現される、エージェントの現在の行動価値関数（Q関数）のモデル。<br>エージェントが現在の状態を入力とし、各行動のQ値を推定するために使用。具体的には、深層ニューラルネットワークがこのモデルとして機能し、状態を入力として受け取り、Q値を出力。エージェントの行動選択に使用される重要な要素。エージェントは、現在の状態に基づいてこのモデルを介して各行動のQ値を推定し、ε-greedy法などの方策に従って行動を選択。`},
{target:`ターゲットモデル`, content:`エージェントの目標とする行動価値関数（Q関数）のモデル。<br>オンラインモデルとは独立した別のネットワークであり、行動価値関数の目標となる値を推定するために使用。具体的には、このモデルはエージェントの現在の状態を入力とし、各行動のQ値を出力。オンラインモデルとは異なるパラメータを持つ。定期的な間隔でオンラインモデルのパラメータをこのモデルにコピーまたは更新することで、このモデルの更新が行われる。これにより、オンラインモデルの変動がこのモデルに直接反映されず、学習の安定性が向上。このモデルは、「オンラインモデル」よりも更新頻度が大幅に低いので、「ターゲットQ値」は安定化し、先ほど触れた「フィードバックループ」は鈍くなり、「フィードバックループ」の悪影響は緩和。DeepMindは、0.00025という非常に小さな「学習率」を使い、10,000ステップを経過しなければこのモデルを更新せず、「100万経験」という非常に大きな「再生バッファ」を使った。`},
{target:`ダブルDQN double DQN`, content:`強化学習のアルゴリズムの一つで、通常のDQN（Deep Q-Network）アルゴリズムの改良版であり、特に行動価値関数の推定において効果を発揮。<br>通常のDQNでは、行動価値関数（Q関数）の更新において、次の状態での最大行動価値を求めるために同じモデルを使用。しかし、これによって高い行動価値が過大評価される問題がある。この問題を軽減するために、行動選択に使用するモデルと行動価値の推定に使用するモデルを分けて利用。具体的には、現在の状態での行動選択には「行動選択用モデル」を使用し、次の状態での行動価値の推定には「行動価値推定用モデル」を使用。これによって、行動選択において高い行動価値が選ばれるのを防ぎ、より安定した学習を促すことができる。`},
{target:`優先度付き経験再生 Prioritized Experience Replay PER 重要度サンプリング Importance Sampling IS`, content:`強化学習のアルゴリズムであるDQN（Deep Q-Network）において、経験の再利用方法を改良する手法。<br>通常の経験再生では、エージェントが過去の経験をランダムにサンプリングして学習に利用。しかし、重要な経験とそうでない経験の間には明確な差が存在する場合がある。このような重要な経験に優先度を付けてサンプリングすることで、学習の効率と性能を向上させることを目指す。経験の優先度を計算するための尺度を導入。一般的には、各経験の予測誤差（TD誤差）を優先度として使用。TD誤差は、オンラインモデルとターゲットモデルの間の差分に基づいて計算。予測誤差が大きい経験は、重要な情報を持つ可能性が高いと考えられ、より高い優先度を持つとする。高い優先度を持つ経験がサンプリングされる確率が高くなり、より重要な経験に対してより多くの学習が行われるため、学習の効率が向上。優先度の計算に基づいてサンプリングされるため、エージェントが重要な経験をより効果的に学習できる。`},
{target:`Dueling DQN Dueling Deep Q Network`, content:`強化学習のアルゴリズムであるDQN（Deep Q-Network）の一変種。価値関数の分解と独立な学習によって、効率的な学習と性能の向上を実現。<br>通常のDQNでは、各状態行動ペアに対して直接的な行動価値（Q値）を推定。しかし、状態自体に対する価値情報と、行動選択による追加の情報を分離して扱うことで、学習の効率や安定性を向上させることができる。価値関数を状態価値とアドバンテージ関数に分解する。状態価値関数は、状態のみに基づいてその価値を評価。これは、状態自体が持つ価値や報酬の期待値を表す。一方、アドバンテージ関数は、行動に関する情報を評価。これは、ある行動が他の行動に比べてどれだけ良いか、あるいは悪いかを示す指標。状態価値関数とアドバンテージ関数を組み合わせることで、各状態行動ペアのQ値を推定。具体的には、状態価値関数の出力と、アドバンテージ関数の出力を組み合わせてQ値を計算。これにより、状態価値と行動の価値情報を独立して学習することができる。利点は、状態価値とアドバンテージ関数を分離することで、学習の効率性と推定の安定性が向上する。また、アドバンテージ関数によって行動価値の差分を推定できるため、行動の重要性をより正確に評価することができる。`},
{target:`TF Agents`, content:`TensorFlowをベースとした強化学習ライブラリ。<br>TF-Agentsは、エージェントの訓練と評価、強化学習アルゴリズムの実装、さまざまな環境との相互作用など、強化学習タスクをサポートする機能を提供。`},
{target:`アクター クリティックアルゴリズム actor critic algorithm`, content:`強化学習の一種であり、方策（アクター）と価値関数（クリティック）を同時に学習する手法。<br>アクターは方策を表し、現在の状態に基づいて行動を選択。クリティックは価値関数を表し、状態や行動の価値を推定。アクターは方策勾配法（Policy Gradient）に基づいて方策を更新し、クリティックは価値関数の更新を行う。方策と価値関数の相互作用により、より効果的な学習が可能。アクターは方策を改善し、クリティックはより正確な価値推定を行うことで、高品質な行動を学習することができる。さまざまなバリエーションが存在し、例としてはAdvantage Actor-Critic （A2C）、Asynchronous Advantage Actor-Critic （A3C）、Proximal Policy Optimization （PPO）などがある。「方策勾配法」と「DQN」を組み合わせた「RLアルゴリズム」のファミリ。「方策ネットワーク」と「DQN」の2つの「ニューラルネットワーク」を含んでいる。「方策ネットワーク」の学習方法は、複数の「エピソード」を通じて個々の「行動」に対する「割引済み」の将来の「報酬」を合計し、「正規化」する。「エージェント」（アクター）は、「DQN」（クリティック）が推計した「行動価値」にもとづいて行動する。これは、「アスリート」（アクター）が「コーチ」（「DQN」）の助けを得て学習していくのと似ている。`},
{target:`A3C Asynchronous Advantage Actor Critic`, content:`アクター・クリティック（Actor-Critic）アルゴリズムの一種。<br>分散型の強化学習手法。複数のエージェント（アクター）が独立に環境とやり取りし、それぞれのエージェントが自身の方策と価値関数を更新することで学習を進める。特徴的な点は、並列化によって高速な学習を実現する。複数のエージェントが同時に環境と対話するため、学習の効率性が向上。各エージェントは自身の経験をもとに方策と価値関数を更新し、それぞれのエージェントが持つグラディエント情報を共有することで、学習の一貫性を確保する。分散型学習の利点を活かしながらも、シンプルで理解しやすいアルゴリズムとなっている。また、複数のエージェントが異なる環境を探索することにより、より幅広い状態空間をカバーすることができる。`},
{target:`A2C Advantage Actor Critic`, content:`アクター・クリティック（Actor-Critic）アルゴリズムの一種であり、強化学習の手法のひとつ。同時に方策（アクター）と価値関数（クリティック）を学習することで、最適な行動選択を実現。<br>一つのエージェントが環境と対話しながら方策と価値関数を同時に更新。方策は状態に対する行動の確率分布を表し、価値関数は状態価値または行動価値を推定。方策は確率的な行動選択を行い、価値関数は状態や行動の価値を評価。特徴的な点は、同時に複数の並列エージェントを使用することで学習の効率性を高めること。各エージェントは自身の経験をもとに方策と価値関数を更新し、それぞれのエージェントが持つ勾配情報を集約することで、学習の一貫性を確保。この並列化により、学習の収束速度を向上させることができる。シンプルで実装しやすいことから広く利用されている。また、REINFORCEアルゴリズムなどの方策勾配法に比べて、低いバイアスと低い分散の勾配推定を行うことができる。これにより、より安定した学習が期待できる。`},
{target:`SAC Soft Actor Critic`, content:`強化学習のアルゴリズムの一つ、特に連続行動空間での学習において高いパフォーマンスを発揮する。<br>方策勾配法（Policy Gradient）とQ学習（Q-Learning）を組み合わせたアプローチを取っている。連続行動空間における方策の学習に強い強化学習手法として知られるDDPG（Deep Deterministic Policy Gradient）からの派生。しかし、DDPGと比較していくつかの改良点を持っている。まず、確率的な方策を採用しており、連続行動空間においても探索を促進。また、連続行動空間における方策の学習にエントロピー項を導入している。エントロピー項は方策の確率分布の多様性を維持し、探索性を高める役割を果たす。さらに、2つのQ関数を使用して行動価値を推定。これにより、Q関数の推定がより安定し、方策の学習も安定化する。また、Q関数の学習にターゲットネットワーク（Target Network）を使用することで、学習の収束性を向上させる。高次元の観測空間や連続行動空間を持つタスクにおいて優れたパフォーマンスを発揮し、安定した学習を実現することができる。さらに、エクスペリエンスリプレイ（Experience Replay）やターゲットネットワークの使用など、他の強化学習手法との組み合わせも可能。そのため、現代の深層強化学習の中でも広く利用されている手法の一つ。「アクター・クリティック」のバリアントで、「報酬」を学習するだけでなく、「行動」の「エントロピー」を最大化する。できる限り多くの「報酬」を得ようとしつつ、できる限り予測不能になろうともする。こうすると、「エージェント」の「環境探索」を後押しして訓練をスピードアップさせ、「DQN」が不完全な推計をしたときに繰り返し同じ「行動」を実行することが減る。この「アルゴリズム」は、見事なサンプリング効率を示す。`},
{target:`PPO Proximal Policy Optimization`, content:`強化学習のアルゴリズムの一つであり、方策勾配法（Policy Gradient）を基礎とした安定した方策最適化手法。<br>特に、連続行動空間や高次元の観測空間での学習において優れた性能を発揮。方策の更新時にクリッピング（clipping）という制約を導入することで、方策の変化を制御し、学習の安定性を向上。具体的には、新しい方策と古い方策との比率を計算し、クリッピング範囲内でのみ方策を更新。これにより、大幅な方策の変化を抑制しながら、古い方策からの学習を可能にする。トラストリージョン（Trust Region）と呼ばれる概念に基づいている。トラストリージョンは、方策の更新幅を制限するための領域を表し、学習の不安定性を抑制する役割を果たす。クリッピングによってトラストリージョンを定義し、方策の更新を制御する。高い学習安定性と収束性を持ちながらも、学習率やハイパーパラメータの調整が比較的容易であり、実装も比較的シンプル。また、サンプル効率が高く、エクスペリエンスリプレイ（Experience Replay）などの補助的な手法を必要としない。最近の強化学習の研究や実装において広く利用されている手法であり、現代の深層強化学習の中でも注目されている手法の一つ。「A2Cベース」で、過度な「重み」更新を避けるように改造された「損失関数」を使う。`},
{target:`好奇心駆動探索 curiosity driven exploration`, content:`強化学習における探索戦略の一つ。エージェントが環境の予測可能性や興味深さに基づいて探索することを重視。<br>従来の探索方法では、報酬最大化を目指して未知の領域を探索することが主な目的だったが、この探索では、エージェントは環境の予測誤差や未知の状態の情報量などを推定し、これらの指標を最大化するような行動を選択。具体的には、エージェントは内部モデルを構築し、そのモデルを使用して環境の未来の状態や報酬を予測。そして、予測の誤差や情報量を報酬として利用し、これを最大化するように学習する。この探索は、報酬信号が疎な環境や高次元の状態空間において有用であり、より効率的な探索を可能にする。また、この探索は、エージェントが新たな知識やスキルを獲得するための自己学習のメカニズムとしても機能する。近年の研究では、この探索を深層強化学習アルゴリズムに組み込んだり、モンテカルロ木探索（Monte Carlo Tree Search）と組み合わせたりするなど、さまざまな応用や改良が行われている。この探索は、未知の領域や困難なタスクにおいて、探索性能の向上や学習効率の改善に寄与する可能性がある。「RL」で繰り返し問題になるのは、「報酬」がまばらにしかないために、学習が非効率で遅くなること。この問題を解決するため、従来の形の「報酬」を無視し、「エージェント」が猛烈な好奇心を持って「環境」を「探索」できるようにする。「報酬」は、「環境」から与えられるものではなく、「エージェント」の本質的な要素になる。「エージェント」は「行動」の結果をいつも予測しようとし続ける。そして、予想に反した結果が出る「状況」を探し求める（びっくりさせられることを追求する）。結果が予測不能でも、「エージェント」が試行錯誤を繰り返して歯が立たないようなら、そのうち飽きてなくなる。好奇心だけで「エージェント」に多くのゲームのプレイ方法を学習させることに成功。`},
{target:`業務専門家`, content:`機械学習などデータ解析でのプロジェクト全般のアイデア出し、入力データ項目の決定など。`},
{target:`データエンジニア`, content:`機械学習などデータ解析でのデータの入手、加工、学習データの準備。`},
{target:`データサイエンティスト`, content:`機械学習などデータ解析でのモデルの作成、モデルの最適化。`},
{target:`RAG Retrieval-Augmented Generation`, content:`LLMを外部のデータベースと連携する手法。<br>これにより最新の情報を参照できるようになる。`},
]
